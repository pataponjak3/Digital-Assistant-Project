DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fa6caef6-f95a-4a0d-8e88-e05961a68f25', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast for 40.7128, -74.0060.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A81015C670>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A81015C3A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:12 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'6eff16e1-9aca-488c-ae3a-c372638674b5'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b031abfbac974-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.403'), (b'fireworks-server-time-to-first-token', b'0.076'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=519.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'404'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'QKyC8TyItEjPaYybGEgR7YwDU8yXV6HxhRt9EKwr1sEz0HPycy6F-w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:12 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '6eff16e1-9aca-488c-ae3a-c372638674b5', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b031abfbac974-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.403', 'fireworks-server-time-to-first-token': '0.076', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=519.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '404', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'QKyC8TyItEjPaYybGEgR7YwDU8yXV6HxhRt9EKwr1sEz0HPycy6F-w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 6eff16e1-9aca-488c-ae3a-c372638674b5
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day forecast for 40.7128, -74.0060.' took 1.22s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 40.7128, 'lon': -74.006})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-19aee296-18b5-4841-b0a0-c1943d690f9a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days near -33.8688, 151.2093?'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A81015E3E0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A81015E170>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:18 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59476'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'd83bd777-ed45-4af5-bb48-59c80036db01'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b033ea8fcc9a0-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'524'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.297'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=557.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'299'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'KKSumuxtT-P_JvF6PCNed7XSEGeSxLzpTebdLZKjO89ebdWbxLNtGQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:18 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59476', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'd83bd777-ed45-4af5-bb48-59c80036db01', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b033ea8fcc9a0-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '524', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.297', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=557.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '299', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'KKSumuxtT-P_JvF6PCNed7XSEGeSxLzpTebdLZKjO89ebdWbxLNtGQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: d83bd777-ed45-4af5-bb48-59c80036db01
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Next 5 days near -33.8688, 151.2093?' took 0.77s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -33.8688, 'lon': 151.2093, 'units': 'standard', 'lang': 'en'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6ebbbb2b-cf40-4723-ba5a-d142d673a08a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast (5d) at -23.5505, -46.6333.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:24 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59475'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'48f85cc5-664e-45c0-bfdb-4acfd5c86cba'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b03627e7ed694-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'525'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.386'), (b'fireworks-server-time-to-first-token', b'0.050'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=490.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'388'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'GB-UCZpC-N2ifRAYKKa3ez49CzsV23OA4nlXlU8vlQU2d1Q8i3o6rw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:24 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59475', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '48f85cc5-664e-45c0-bfdb-4acfd5c86cba', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b03627e7ed694-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '525', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.386', 'fireworks-server-time-to-first-token': '0.050', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=490.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '388', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'GB-UCZpC-N2ifRAYKKa3ez49CzsV23OA4nlXlU8vlQU2d1Q8i3o6rw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 48f85cc5-664e-45c0-bfdb-4acfd5c86cba
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast (5d) at -23.5505, -46.6333.' took 0.67s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -23.5505, 'lon': -46.6333, 'units': 'standard', 'lang': 'en'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-61811d98-d4c4-4332-9158-c8e628053456', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Show me 5-day outlook for 30.0444, 31.2357.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A5BA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A5780>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:29 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59474'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'3acfad40-c5c4-46d7-818d-3ffd4114c21d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b038669640774-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'526'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.203'), (b'fireworks-server-time-to-first-token', b'0.039'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=336.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'205'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 735dd7fa3c296627cd805d04e3e3a3e8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'wcr7EMO2Idw4vURe7kVnQuc_IuSoGPX_85iDnH3rwuGlmaMw4aFTww==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:29 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59474', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '3acfad40-c5c4-46d7-818d-3ffd4114c21d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b038669640774-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '526', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.203', 'fireworks-server-time-to-first-token': '0.039', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=336.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '205', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 735dd7fa3c296627cd805d04e3e3a3e8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'wcr7EMO2Idw4vURe7kVnQuc_IuSoGPX_85iDnH3rwuGlmaMw4aFTww=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 3acfad40-c5c4-46d7-818d-3ffd4114c21d
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Show me 5-day outlook for 30.0444, 31.2357.' took 0.61s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 30.0444, 'lon': 31.2357})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-95df8561-ce06-4462-9e63-ca9c097cee9f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the week ahead at 55.7558, 37.6173?"}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A78B0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A7490>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:35 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59475'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'04bc5a3f-264a-4709-91f8-72441a33fd8c'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b03a968325b46-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'525'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.344'), (b'fireworks-server-time-to-first-token', b'0.051'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=549.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'346'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'9HzhAg7V91a4ucxGbFxcVSdBG8T-yRltkDWLwR28sGJJnmuhOsNw7g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:35 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59475', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '04bc5a3f-264a-4709-91f8-72441a33fd8c', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b03a968325b46-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '525', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.344', 'fireworks-server-time-to-first-token': '0.051', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=549.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '346', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '9HzhAg7V91a4ucxGbFxcVSdBG8T-yRltkDWLwR28sGJJnmuhOsNw7g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 04bc5a3f-264a-4709-91f8-72441a33fd8c
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'What's the week ahead at 55.7558, 37.6173?' took 0.79s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 55.7558, 'lon': 37.6173})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d46809cb-6abb-4255-a2dd-be54c3401019', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day forecast around -1.2921, 36.8219.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A5900>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A5210>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:41 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'af55ed5b-967e-42bf-8553-ae4d14cbd6c4'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b03cda971c3b1-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.329'), (b'fireworks-server-time-to-first-token', b'0.054'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=421.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'331'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'gINP65IkN0pLve_KF-nJc50CIKqulA7NHs06Mp9--F1SKWTVTCI74w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:41 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'af55ed5b-967e-42bf-8553-ae4d14cbd6c4', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b03cda971c3b1-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.329', 'fireworks-server-time-to-first-token': '0.054', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=421.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '331', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'gINP65IkN0pLve_KF-nJc50CIKqulA7NHs06Mp9--F1SKWTVTCI74w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: af55ed5b-967e-42bf-8553-ae4d14cbd6c4
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Five-day forecast around -1.2921, 36.8219.' took 0.64s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -1.2921, 'lon': 36.8219})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a8dbd520-85be-497f-a3f5-45882c967338', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Is it cooling later this week near 64.1466, -21.9426?'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D0DC0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D09A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:46 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59474'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'87e0095a-4a7f-4503-b8fa-b291eeb4c8b0'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b03f0f8355764-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'526'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.248'), (b'fireworks-server-time-to-first-token', b'0.084'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=371.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'250'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'rIicYCa34pz9z5G9aw595R_wEC_jvCDuuKvrg6CXQfi5YuG11ZsQtA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:46 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59474', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '87e0095a-4a7f-4503-b8fa-b291eeb4c8b0', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b03f0f8355764-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '526', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.248', 'fireworks-server-time-to-first-token': '0.084', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=371.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '250', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'rIicYCa34pz9z5G9aw595R_wEC_jvCDuuKvrg6CXQfi5YuG11ZsQtA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 87e0095a-4a7f-4503-b8fa-b291eeb4c8b0
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Is it cooling later this week near 64.1466, -21.9426?' took 0.61s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 64.1466, 'lon': -21.9426})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-db2c92a8-47fa-4aa4-b34d-64cd1aa364df', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Give 5-day hi/lo for -33.9249, 18.4241.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D2AD0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D26B0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:52 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59474'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'7414feeb-623c-4a83-a7d6-c6d950394455'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0414296bd8b4-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'526'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.172'), (b'fireworks-server-time-to-first-token', b'0.039'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=275.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'173'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'3qTD--MOW8yU4fHjujUfgT-h2Ol2h53xSYelOUTlFXQ6shmASSZZ5A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:52 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59474', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '7414feeb-623c-4a83-a7d6-c6d950394455', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0414296bd8b4-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '526', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.172', 'fireworks-server-time-to-first-token': '0.039', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=275.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '173', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '3qTD--MOW8yU4fHjujUfgT-h2Ol2h53xSYelOUTlFXQ6shmASSZZ5A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 7414feeb-623c-4a83-a7d6-c6d950394455
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Give 5-day hi/lo for -33.9249, 18.4241.' took 0.51s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -33.9249, 'lon': 18.4241})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7d24daf7-33c2-46b5-a7f6-3ff7dcf278d1', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast next days at 35.6895, 139.6917.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F0820>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F0400>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:54:58 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'88bef065-345c-47d8-8c03-d37f0f3c2b7c'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0436aa79a504-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.349'), (b'fireworks-server-time-to-first-token', b'0.049'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=444.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'351'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 68c9162ccc29f8f3ca30be36950cea58.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Sdtuq1P6YqjKn2nRfli_QfaTgajtlfzNvt5XL39IyZvTNSfqaLjaHg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:54:58 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '88bef065-345c-47d8-8c03-d37f0f3c2b7c', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0436aa79a504-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.349', 'fireworks-server-time-to-first-token': '0.049', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=444.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '351', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 68c9162ccc29f8f3ca30be36950cea58.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Sdtuq1P6YqjKn2nRfli_QfaTgajtlfzNvt5XL39IyZvTNSfqaLjaHg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 88bef065-345c-47d8-8c03-d37f0f3c2b7c
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast next days at 35.6895, 139.6917.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 35.6895, 'lon': 139.6917})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b8ff330b-5984-4d01-9a2c-f1e20fe3d64f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day for 38.7223, -9.1393 (Lisbon).'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D3DC0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101D3D30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:04 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59474'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'149332e0-2674-40d2-86c2-c48816cce84d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b045c9f0a7fe7-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'526'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.292'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=430.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'294'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'hh667spVIHFHnEEplJD_UJINag_vVIERWsuGjoquMSjtl6qcKZ0j-Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:04 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59474', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '149332e0-2674-40d2-86c2-c48816cce84d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b045c9f0a7fe7-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '526', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.292', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=430.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '294', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'hh667spVIHFHnEEplJD_UJINag_vVIERWsuGjoquMSjtl6qcKZ0j-Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 149332e0-2674-40d2-86c2-c48816cce84d
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Five-day for 38.7223, -9.1393 (Lisbon).' took 1.06s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 38.7223, 'lon': -9.1393, 'units': 'standard'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6804429d-f182-4639-9802-357b6c3d35e2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast 10001, US.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A40A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A81015E110>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:09 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'fb5e4602-2091-4ffe-a5d0-ea4da4b68128'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b04805aee6c71-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.231'), (b'fireworks-server-time-to-first-token', b'0.037'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=475.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'233'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'jbmDOCq2xiQ84nUBpheoa26gg2TFRsmtItOPraun_Nzc4-t3m4vJng==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:09 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'fb5e4602-2091-4ffe-a5d0-ea4da4b68128', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b04805aee6c71-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.231', 'fireworks-server-time-to-first-token': '0.037', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=475.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '233', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'jbmDOCq2xiQ84nUBpheoa26gg2TFRsmtItOPraun_Nzc4-t3m4vJng=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: fb5e4602-2091-4ffe-a5d0-ea4da4b68128
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day forecast 10001, US.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '10001', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-38e435a0-12d2-4347-a34d-1175f8abe1dc', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast for 90210, US next 5 days.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F2950>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F2530>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:15 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59481'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'a1fd926f-9194-4b8e-8b5c-b3c51a966f2d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b04a459d8c981-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'519'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.296'), (b'fireworks-server-time-to-first-token', b'0.104'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=621.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'297'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'ber_D79Qo11tj4BDWX2Vyk3bLWY0oig7wiJ-ycblGBEiidN9aBe-3g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:15 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59481', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'a1fd926f-9194-4b8e-8b5c-b3c51a966f2d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b04a459d8c981-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '519', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.296', 'fireworks-server-time-to-first-token': '0.104', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=621.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '297', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'ber_D79Qo11tj4BDWX2Vyk3bLWY0oig7wiJ-ycblGBEiidN9aBe-3g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: a1fd926f-9194-4b8e-8b5c-b3c51a966f2d
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast for 90210, US next 5 days.' took 0.89s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '90210', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f156599c-9d4c-4a87-aaf8-5edb6fd3f37d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Weather outlook 60614, US.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8102106A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810210280>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:21 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'43cb6e6d-441b-4d7b-8ce9-6e99bfd19c14'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b04c8ded4d65d-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.184'), (b'fireworks-server-time-to-first-token', b'0.041'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=351.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'185'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'2KvCYM6v0Ci4AuK1I2uZ9y-6Mr5c45_cSQFYq64wDU3ye6HwQ6mGdQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:21 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '43cb6e6d-441b-4d7b-8ce9-6e99bfd19c14', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b04c8ded4d65d-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.184', 'fireworks-server-time-to-first-token': '0.041', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=351.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '185', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '2KvCYM6v0Ci4AuK1I2uZ9y-6Mr5c45_cSQFYq64wDU3ye6HwQ6mGdQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 43cb6e6d-441b-4d7b-8ce9-6e99bfd19c14
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Weather outlook 60614, US.' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_current_weather', module='meteorology', arguments={'zip': '60614', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-731d52d5-ffa4-4f95-af72-5f7993491bbe', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for 10115, DE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:27 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'0a20026d-5d1d-4813-ab92-451c060022a7'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b04eb99586529-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.271'), (b'fireworks-server-time-to-first-token', b'0.082'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=377.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'272'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'r3Fhx2FpY6LVsjyLnb7LU5yEYa_jWFd6uiRvgv2mmoV7XSJMmtbP3g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:27 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '0a20026d-5d1d-4813-ab92-451c060022a7', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b04eb99586529-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.271', 'fireworks-server-time-to-first-token': '0.082', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=377.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '272', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'r3Fhx2FpY6LVsjyLnb7LU5yEYa_jWFd6uiRvgv2mmoV7XSJMmtbP3g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 0a20026d-5d1d-4813-ab92-451c060022a7
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day for 10115, DE.' took 0.55s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '10115', 'country_code': 'DE', 'units': 'standard', 'lang': 'en'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e88c1e03-8745-4091-9b97-cb96bd1561c1', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next five days SW1A 1AA, GB.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A5C90>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101A56C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:32 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59481'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'dfb40100-95c4-4a91-bee9-63d0f6c6d450'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b050ebb39d6a1-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'519'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.207'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=380.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'210'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e33c092c8429c324c2810046683bf2f2.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'OLrE3Y7TEK1hM8uwPOR4bBZHyYNa9Qt7wmOyoIiwbhJo94Ymfg4fXg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:32 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59481', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'dfb40100-95c4-4a91-bee9-63d0f6c6d450', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b050ebb39d6a1-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '519', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.207', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=380.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '210', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e33c092c8429c324c2810046683bf2f2.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'OLrE3Y7TEK1hM8uwPOR4bBZHyYNa9Qt7wmOyoIiwbhJo94Ymfg4fXg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: dfb40100-95c4-4a91-bee9-63d0f6c6d450
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Next five days SW1A 1AA, GB.' took 0.62s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': 'SW1A 1AA', 'country_code': 'GB'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0ed6108b-20c3-47c3-84a9-4a5f4a882088', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast 75001, FR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F1C00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F26B0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:38 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'0f008ebb-1511-4c78-a1a3-7a2551a58742'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0531d9518245-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.548'), (b'fireworks-server-time-to-first-token', b'0.180'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=671.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'561'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 68c9162ccc29f8f3ca30be36950cea58.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'C4ChOrZ9LNz903OYDTLsD4pQ2t5Rlhybriz9SrX4HdEYXwrxF2wiew==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:38 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '0f008ebb-1511-4c78-a1a3-7a2551a58742', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0531d9518245-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.548', 'fireworks-server-time-to-first-token': '0.180', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=671.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '561', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 68c9162ccc29f8f3ca30be36950cea58.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'C4ChOrZ9LNz903OYDTLsD4pQ2t5Rlhybriz9SrX4HdEYXwrxF2wiew=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 0f008ebb-1511-4c78-a1a3-7a2551a58742
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast 75001, FR.' took 0.90s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '75001', 'country_code': 'FR'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b235ccfa-90b7-4747-8732-b96c0c59b02d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for 1250-096, PT.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810210100>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810211930>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:44 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59482'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'edf6f80d-b70a-4dc8-9ce5-10a9bbef07c4'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0556dc8aa9d6-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'518'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.367'), (b'fireworks-server-time-to-first-token', b'0.048'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=460.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'369'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'zHV_6pw6zU4U_8jja9-vYNiec2RyB5fAvpw85Ldqd9wj_PGGqh9J8w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:44 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59482', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'edf6f80d-b70a-4dc8-9ce5-10a9bbef07c4', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0556dc8aa9d6-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '518', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.367', 'fireworks-server-time-to-first-token': '0.048', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=460.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '369', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'zHV_6pw6zU4U_8jja9-vYNiec2RyB5fAvpw85Ldqd9wj_PGGqh9J8w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: edf6f80d-b70a-4dc8-9ce5-10a9bbef07c4
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day for 1250-096, PT.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '1250-096', 'country_code': 'PT', 'units': 'standard', 'lang': 'pt'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5db3cf2f-817b-422c-bf62-007c745242a4', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast for 2000, AU.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8102413F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810240FD0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:49 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'ecc205df-81ca-4953-ab47-5e16dd2ef11a'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b057aaee1e5f7-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.184'), (b'fireworks-server-time-to-first-token', b'0.041'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=283.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'185'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 8b0fd9a74745a3290a9c7ffef7d75076.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'a2IbGrogO-G0LHXXwXsgZ6Kc60fxT3k30rgs0G9VMidgLVKlI_WLLQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:49 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'ecc205df-81ca-4953-ab47-5e16dd2ef11a', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b057aaee1e5f7-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.184', 'fireworks-server-time-to-first-token': '0.041', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=283.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '185', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 8b0fd9a74745a3290a9c7ffef7d75076.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'a2IbGrogO-G0LHXXwXsgZ6Kc60fxT3k30rgs0G9VMidgLVKlI_WLLQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: ecc205df-81ca-4953-ab47-5e16dd2ef11a
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast for 2000, AU.' took 0.52s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': '2000', 'country_code': 'AU'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-771afb9f-7867-43c3-b5ef-2fd1f75eb108', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day outlook 01000-000, BR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810243100>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810242CE0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:55:55 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59482'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'62553bfe-bb1d-42df-a848-b5eca33fd6b6'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b059d2ac8a177-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'518'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.442'), (b'fireworks-server-time-to-first-token', b'0.060'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=578.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'444'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'biYUkgPAeqFI_ehX-0bP-jA4121HagcVkNUQvIsfCJAsfCMqQXvnBA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:55:55 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59482', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '62553bfe-bb1d-42df-a848-b5eca33fd6b6', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b059d2ac8a177-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '518', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.442', 'fireworks-server-time-to-first-token': '0.060', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=578.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '444', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'biYUkgPAeqFI_ehX-0bP-jA4121HagcVkNUQvIsfCJAsfCMqQXvnBA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 62553bfe-bb1d-42df-a848-b5eca33fd6b6
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day outlook 01000-000, BR.' took 0.81s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'São Paulo', 'country_code': 'BR', 'zip': '01000-000'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-119660d5-9e25-497a-add4-afdbfde12187', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days 110001, IN.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:01 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59483'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'6b9986ca-2904-48a0-950e-51c433de2b0d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b05c14df81de9-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'517'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.240'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=469.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'242'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'QeJA9YbY9c8Zg7oUs6lm5gLokeRln2Heo2gEwsyWA4vvQ7N5h0oD7g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:01 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59483', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '6b9986ca-2904-48a0-950e-51c433de2b0d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b05c14df81de9-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '517', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.240', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=469.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '242', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'QeJA9YbY9c8Zg7oUs6lm5gLokeRln2Heo2gEwsyWA4vvQ7N5h0oD7g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 6b9986ca-2904-48a0-950e-51c433de2b0d
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Next 5 days 110001, IN.' took 0.66s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '110001', 'country_code': 'US', 'state_code': 'IN'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-27996a7b-7ce9-44af-ac57-633b81cf1a2d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day forecast Kyoto, JP.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F1DE0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8101F21D0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:06 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9888d7ee-3827-40d3-9cc6-b93169fab4a4'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b05e4ec1cc999-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.286'), (b'fireworks-server-time-to-first-token', b'0.050'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=394.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'288'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'eQ_3TNArj0TN5u5t2SmVmsjdRnd-etWb7Z5reS_-S4JQr2ZfXyMzXA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:06 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9888d7ee-3827-40d3-9cc6-b93169fab4a4', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b05e4ec1cc999-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.286', 'fireworks-server-time-to-first-token': '0.050', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=394.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '288', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'eQ_3TNArj0TN5u5t2SmVmsjdRnd-etWb7Z5reS_-S4JQr2ZfXyMzXA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9888d7ee-3827-40d3-9cc6-b93169fab4a4
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Five-day forecast Kyoto, JP.' took 0.61s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Kyoto', 'country_code': 'JP'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-bea4d0d6-8d20-489f-8a60-56bba673e126', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the 5-day in Toronto, CA?"}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810213A60>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8102120B0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:12 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59482'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9d6b0792-1de9-4bad-a626-675c6e98f427'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b06081e8eea42-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'518'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.319'), (b'fireworks-server-time-to-first-token', b'0.052'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=504.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'320'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'HQsPITtcY_x5hNYBWefGNhCPW4-9ToK867lNqFXjnJ5msF1as53U8w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:12 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59482', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9d6b0792-1de9-4bad-a626-675c6e98f427', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b06081e8eea42-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '518', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.319', 'fireworks-server-time-to-first-token': '0.052', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=504.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '320', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'HQsPITtcY_x5hNYBWefGNhCPW4-9ToK867lNqFXjnJ5msF1as53U8w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9d6b0792-1de9-4bad-a626-675c6e98f427
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'What's the 5-day in Toronto, CA?' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Toronto', 'country_code': 'CA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5c4e57a3-20ae-4d1e-95b3-aa37104cd1b1', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast Nairobi.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810256110>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810255CF0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:18 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59488'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'46de993a-db4a-4a18-895f-600373d058f8'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b062c6910d637-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'512'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.333'), (b'fireworks-server-time-to-first-token', b'0.086'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=526.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'334'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'8TWFot3eaY5H_XPFFLkBfwCI8ggQYgCqfRnb682t4EqdS0ni7qI6Yg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:18 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59488', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '46de993a-db4a-4a18-895f-600373d058f8', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b062c6910d637-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '512', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.333', 'fireworks-server-time-to-first-token': '0.086', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=526.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '334', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '8TWFot3eaY5H_XPFFLkBfwCI8ggQYgCqfRnb682t4EqdS0ni7qI6Yg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 46de993a-db4a-4a18-895f-600373d058f8
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day forecast Nairobi.' took 0.80s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Nairobi'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ea7f5330-0ba9-4adb-a19d-1750e1c55793', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Week ahead in Auckland, NZ.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:24 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'955e4bbf-5da7-46fb-b0cd-3f24b2b63d2f'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b06501995dab8-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.329'), (b'fireworks-server-time-to-first-token', b'0.065'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=427.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'331'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'iaq55AwuErLsIeJ07WD0NrANZtsLT63DyLbioTmBrpb9YhUwOJ6bgw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:24 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '955e4bbf-5da7-46fb-b0cd-3f24b2b63d2f', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b06501995dab8-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.329', 'fireworks-server-time-to-first-token': '0.065', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=427.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '331', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'iaq55AwuErLsIeJ07WD0NrANZtsLT63DyLbioTmBrpb9YhUwOJ6bgw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 955e4bbf-5da7-46fb-b0cd-3f24b2b63d2f
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Week ahead in Auckland, NZ.' took 0.61s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Auckland', 'country_code': 'NZ'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4ae1aab3-214a-43f6-89a1-d6cd053d2788', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for Reykjavik, IS (typo earlier).'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810271750>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810271330>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:29 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59479'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c54f9184-d1a3-4cf4-8be4-5525cba81d9e'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b06739d32c095-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'521'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.221'), (b'fireworks-server-time-to-first-token', b'0.037'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=353.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'223'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'xvaI7d3OgoeJMJwQ-jwLrbHY5tranvhr4jaVlxCddvGOiW-iWh69_Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:29 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59479', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c54f9184-d1a3-4cf4-8be4-5525cba81d9e', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b06739d32c095-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '521', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.221', 'fireworks-server-time-to-first-token': '0.037', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=353.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '223', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'xvaI7d3OgoeJMJwQ-jwLrbHY5tranvhr4jaVlxCddvGOiW-iWh69_Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c54f9184-d1a3-4cf4-8be4-5525cba81d9e
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day for Reykjavik, IS (typo earlier).' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Reykjavik', 'country_code': 'IS', 'units': 'standard'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-55fd15a9-002e-4a25-a32c-99a16535ddc9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast 5 days Lima, PE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810257640>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810255A50>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:35 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c74388ec-150e-433e-b0bd-cdb9a2a609b5'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0696abafd6a8-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.281'), (b'fireworks-server-time-to-first-token', b'0.039'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=611.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'283'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e33c092c8429c324c2810046683bf2f2.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Hl5rDkg0YilbOb4ot0fAtDI6mXhSIlCcdB70gdEXl492A-MoNImJwg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:35 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c74388ec-150e-433e-b0bd-cdb9a2a609b5', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0696abafd6a8-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.281', 'fireworks-server-time-to-first-token': '0.039', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=611.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '283', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e33c092c8429c324c2810046683bf2f2.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Hl5rDkg0YilbOb4ot0fAtDI6mXhSIlCcdB70gdEXl492A-MoNImJwg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c74388ec-150e-433e-b0bd-cdb9a2a609b5
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Forecast 5 days Lima, PE.' took 0.85s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Lima', 'country_code': 'PE', 'units': 'standard'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a1c086ef-1e61-402c-b15c-56182974f886', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days Johannesburg, ZA.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810240CA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810241120>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:41 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'8cc2fe51-bd1d-4b70-8dbe-10d1a3e63323'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b06bb2c4b38a9-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.250'), (b'fireworks-server-time-to-first-token', b'0.051'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=454.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'252'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'BloEnNIqzyZthmso3vc3TUv8-nufUBWWuaeCkROD3hP3bnbEcoD9xw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:41 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '8cc2fe51-bd1d-4b70-8dbe-10d1a3e63323', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b06bb2c4b38a9-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.250', 'fireworks-server-time-to-first-token': '0.051', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=454.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '252', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'BloEnNIqzyZthmso3vc3TUv8-nufUBWWuaeCkROD3hP3bnbEcoD9xw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 8cc2fe51-bd1d-4b70-8dbe-10d1a3e63323
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Next 5 days Johannesburg, ZA.' took 0.70s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Johannesburg', 'country_code': 'ZA', 'units': 'standard', 'lang': 'en'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-83ce4d31-9de3-4fba-a20a-84a3bb7cf718', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Munich, DE 5-day outlook.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810272DD0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A8102729B0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:47 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59483'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'6acb6487-ca8d-4d5f-a32c-ed35a30082d0'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b06dee883a504-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'517'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.332'), (b'fireworks-server-time-to-first-token', b'0.055'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=593.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'334'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'G67HtrNMMPUYdBK69gywIW_nTovFVzJDwiVtM6HgovZ7jfHGrx-F_g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:47 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59483', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '6acb6487-ca8d-4d5f-a32c-ed35a30082d0', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b06dee883a504-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '517', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.332', 'fireworks-server-time-to-first-token': '0.055', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=593.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '334', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'G67HtrNMMPUYdBK69gywIW_nTovFVzJDwiVtM6HgovZ7jfHGrx-F_g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 6acb6487-ca8d-4d5f-a32c-ed35a30082d0
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'Munich, DE 5-day outlook.' took 0.84s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Munich', 'country_code': 'DE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-12a5cb35-16c0-4622-b429-84db1cf112ba', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'San Francisco, US-CA 5-day forecast.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810294B20>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810294700>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:52 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59482'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'f6784d58-b192-4e35-a23e-4c2818bed792'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b07038e2f1ff1-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'518'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.189'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=397.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'191'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5a9df1bcd5f48109e94a8e34d807b686.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'm8r_2rANlG4Z6Scpvlzk10ajTVKGMXAr4Tlkb8yvFYsfaOjk0fNOpA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:52 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59482', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'f6784d58-b192-4e35-a23e-4c2818bed792', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b07038e2f1ff1-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '518', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.189', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=397.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '191', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5a9df1bcd5f48109e94a8e34d807b686.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'm8r_2rANlG4Z6Scpvlzk10ajTVKGMXAr4Tlkb8yvFYsfaOjk0fNOpA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: f6784d58-b192-4e35-a23e-4c2818bed792
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation 'San Francisco, US-CA 5-day forecast.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'San Francisco', 'state_code': 'US-CA', 'units': 'standard'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d197429d-5bb2-467c-a2ba-5e6769821984', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for Porto, PT.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810242F80>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001A80D3A98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001A810243250>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:56:58 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'7cb36ce7-86c6-45b1-811a-0180e8fb39bd'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0726f96a87a9-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.161'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=275.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'163'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 a8c46277c4df8eb178aecf68b391d6de.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'RvAxjgoc7sSPMLtzSB42AGjllhyxJ2zv0zue9tyLnLOYi3_X_8EI4A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:56:58 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '7cb36ce7-86c6-45b1-811a-0180e8fb39bd', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0726f96a87a9-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.161', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=275.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '163', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 a8c46277c4df8eb178aecf68b391d6de.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'RvAxjgoc7sSPMLtzSB42AGjllhyxJ2zv0zue9tyLnLOYi3_X_8EI4A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 7cb36ce7-86c6-45b1-811a-0180e8fb39bd
DEBUG    test_performance:test_performance.py:452 5 day forecast conversation '5-day for Porto, PT.' took 0.54s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Porto', 'country_code': 'PT'})


