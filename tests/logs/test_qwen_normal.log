DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ea03b1a6-75d8-433c-adcb-bb4e59b9191f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Explain the difference between threads and processes in one paragraph.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20953C40>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20953970>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:55:32 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaFE8-62bZhn-983404602d0682cf'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983404602d0682cf-IAD'), (b'etag', b'W/"366-/0+MqTaiHqjHhwCqEJpBs+YJaKQ"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:55:32.515Z'), (b'x-api-call-start', b'2025-09-22T18:55:31.968Z'), (b'x-api-received', b'2025-09-22T18:55:31.961Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 0d7deb275c6269c1898f9fac0cd690b6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'POX_kjrGS7mOWjlDE3uKqSW0Sqcbqg2wRJFdf6RG1BwkL__wgiw8Xw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:55:32 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaFE8-62bZhn-983404602d0682cf', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983404602d0682cf-IAD', 'etag': 'W/"366-/0+MqTaiHqjHhwCqEJpBs+YJaKQ"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:55:32.515Z', 'x-api-call-start': '2025-09-22T18:55:31.968Z', 'x-api-received': '2025-09-22T18:55:31.961Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 0d7deb275c6269c1898f9fac0cd690b6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'POX_kjrGS7mOWjlDE3uKqSW0Sqcbqg2wRJFdf6RG1BwkL__wgiw8Xw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaFE8-62bZhn-983404602d0682cf
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Explain the difference between threads and processes in one paragraph.' took 2.30s with response: 
LLMResponse(type='response', content='In computer science, a process is an instance of a program that is being executed. Each process has its own memory space and resources. Threads, on the other hand, are lighter-weight than processes and share the same memory space. They are parts of a process that can run concurrently, allowing for parallel execution within the same program.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4eb65ecf-97e1-4e01-9fc5-924b705e4d6b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Give me 3 meal prep ideas for a vegetarian who trains for half-marathons.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20991BA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20991630>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:55:38 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaGzq-62bZhn-983404859db091fa'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983404859db091fa-IAD'), (b'etag', b'W/"3d8-H+f8+vf2ooRkkn/iI943FrahS68"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:55:38.721Z'), (b'x-api-call-start', b'2025-09-22T18:55:37.926Z'), (b'x-api-received', b'2025-09-22T18:55:37.918Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5326a5d83b2438df3791f26526ec3dee.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'8tAee9kTzIBrCAJiQR1Lkmhc0rjxQ53FA19z4BJVl9yUNmj0qphj8w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:55:38 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaGzq-62bZhn-983404859db091fa', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983404859db091fa-IAD', 'etag': 'W/"3d8-H+f8+vf2ooRkkn/iI943FrahS68"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:55:38.721Z', 'x-api-call-start': '2025-09-22T18:55:37.926Z', 'x-api-received': '2025-09-22T18:55:37.918Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5326a5d83b2438df3791f26526ec3dee.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '8tAee9kTzIBrCAJiQR1Lkmhc0rjxQ53FA19z4BJVl9yUNmj0qphj8w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaGzq-62bZhn-983404859db091fa
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Give me 3 meal prep ideas for a vegetarian who trains for half-marathons.' took 1.19s with response: 
LLMResponse(type='response', content="I can suggest meal prep ideas, but I don't have a function to generate them directly. Here are a few vegetarian meal prep ideas that are good for someone training for a half-marathon:\n\n1. Quinoa Salad with Chickpeas, Vegetables, and Lemon Dressing\n2. Sweet Potato and Black Bean Tacos with Avocado Salsa\n3. Lentil Soup with Spinach and Whole Grain Bread\n\nThese meals are packed with nutrients and can help support your training. Enjoy your meal prep!", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4cc418ba-410a-439e-bd67-af01c1dca25b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Summarize the key idea of 'inversion' from mental models (keep it short)."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20993A30>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20993610>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:55:44 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaJsP-66dFFu-983404ac4a993450'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983404ac4a993450-IAD'), (b'etag', b'W/"288-L5misJ8yniC2AYeJT/cDV9p8hc8"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:55:44.593Z'), (b'x-api-call-start', b'2025-09-22T18:55:44.225Z'), (b'x-api-received', b'2025-09-22T18:55:44.215Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 ac3060b504d45421db0adf8239900fec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'CPgvodztCtfYw4VZJut5kC4saroinADFfCAAykf_BpdC2JZVgeyHZw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:55:44 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaJsP-66dFFu-983404ac4a993450', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983404ac4a993450-IAD', 'etag': 'W/"288-L5misJ8yniC2AYeJT/cDV9p8hc8"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:55:44.593Z', 'x-api-call-start': '2025-09-22T18:55:44.225Z', 'x-api-received': '2025-09-22T18:55:44.215Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 ac3060b504d45421db0adf8239900fec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'CPgvodztCtfYw4VZJut5kC4saroinADFfCAAykf_BpdC2JZVgeyHZw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaJsP-66dFFu-983404ac4a993450
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Summarize the key idea of 'inversion' from mental models (keep it short).' took 0.93s with response: 
LLMResponse(type='response', content='Inversion is thinking about a problem by considering its opposite or what to avoid, rather than what to pursue directly.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-971ed05a-9251-4654-9cb0-2c991335e1e8', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What are common pitfalls when using Python's asyncio with CPU-bound work?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209936A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20993070>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:55:51 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaLcJ-66dFFu-983404d17f853afa'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983404d17f853afa-IAD'), (b'etag', b'W/"44d-8rM1eAMwOxSVAcceseWMpc7a9rQ"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:55:51.003Z'), (b'x-api-call-start', b'2025-09-22T18:55:50.077Z'), (b'x-api-received', b'2025-09-22T18:55:50.067Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 ac3060b504d45421db0adf8239900fec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'Oh3qF3DbVoYq4tsblbX6Rp7-y3sLis4X5gadChttk1X8Na8rJRoOGA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:55:51 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaLcJ-66dFFu-983404d17f853afa', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983404d17f853afa-IAD', 'etag': 'W/"44d-8rM1eAMwOxSVAcceseWMpc7a9rQ"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:55:51.003Z', 'x-api-call-start': '2025-09-22T18:55:50.077Z', 'x-api-received': '2025-09-22T18:55:50.067Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 ac3060b504d45421db0adf8239900fec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'Oh3qF3DbVoYq4tsblbX6Rp7-y3sLis4X5gadChttk1X8Na8rJRoOGA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaLcJ-66dFFu-983404d17f853afa
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What are common pitfalls when using Python's asyncio with CPU-bound work?' took 1.30s with response: 
LLMResponse(type='response', content="When using Python's asyncio with CPU-bound work, one common pitfall is not properly identifying tasks that are truly I/O-bound versus CPU-bound. asyncio is designed to handle I/O-bound and high-latency operations efficiently, but it may not provide performance benefits for CPU-bound tasks. Another pitfall is improper use of the `run_in_executor` method, which can lead to unexpected behavior if not managed correctly. Additionally, failing to use the `asyncio.run()` or `asyncio.get_event_loop()` properly can cause issues with the event loop's management and lifecycle.", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cf6a6643-4ade-4b85-9a2a-3980cdda74ad', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Draft a friendly email asking for a project deadline extension.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F1A50>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F1630>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:55:57 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaNc8-3NKUce-983404f8ee10c9b8'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983404f8ee10c9b8-IAD'), (b'etag', b'W/"43d-b+YCqZMMM92V6Gy1voW80BliKJw"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:55:57.928Z'), (b'x-api-call-start', b'2025-09-22T18:55:56.800Z'), (b'x-api-received', b'2025-09-22T18:55:56.785Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 feb24448bf4558d4eaf8be9cf460ac86.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'PLeq_g_ZgrTagW0k2Egqd4qDdoWkvnA_hqGgdfxKyDCIA2WcEHDoJg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:55:57 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaNc8-3NKUce-983404f8ee10c9b8', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983404f8ee10c9b8-IAD', 'etag': 'W/"43d-b+YCqZMMM92V6Gy1voW80BliKJw"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:55:57.928Z', 'x-api-call-start': '2025-09-22T18:55:56.800Z', 'x-api-received': '2025-09-22T18:55:56.785Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 feb24448bf4558d4eaf8be9cf460ac86.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'PLeq_g_ZgrTagW0k2Egqd4qDdoWkvnA_hqGgdfxKyDCIA2WcEHDoJg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaNc8-3NKUce-983404f8ee10c9b8
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Draft a friendly email asking for a project deadline extension.' took 1.49s with response: 
LLMResponse(type='function_call', content=None, function='compose_email', module='email', arguments={'subject': 'Request for Project Deadline Extension', 'body': 'Dear Team,\n\nI hope this message finds you well. I am writing to request an extension on the project deadline. Due to unforeseen circumstances, our team is facing some challenges that are impacting our progress. We believe a two-week extension would allow us to complete the project to the best of our abilities.\n\nThank you for your understanding.\nBest regards,\n[Your Name]'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5b2b459f-5eeb-43ef-9e4f-6eacad0c0ad2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's a good daily warm-up routine for desk workers? Keep it under 8 steps."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F37C0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F33A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:05 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaQRR-66dFFu-983405219d8220b1'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983405219d8220b1-IAD'), (b'etag', b'W/"633-UXpkyhnRWRXLzz7ZqLoENWvIriw"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:05.635Z'), (b'x-api-call-start', b'2025-09-22T18:56:02.904Z'), (b'x-api-received', b'2025-09-22T18:56:02.893Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 0d20bf93d01cd79d3eec10e42e06bb3c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'WxEYjQGbzmndO7R7lh8L6Q8kBzgpK0xR3nxUnobGI3wQEUgiQ4yuvQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:05 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaQRR-66dFFu-983405219d8220b1', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983405219d8220b1-IAD', 'etag': 'W/"633-UXpkyhnRWRXLzz7ZqLoENWvIriw"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:05.635Z', 'x-api-call-start': '2025-09-22T18:56:02.904Z', 'x-api-received': '2025-09-22T18:56:02.893Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 0d20bf93d01cd79d3eec10e42e06bb3c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'WxEYjQGbzmndO7R7lh8L6Q8kBzgpK0xR3nxUnobGI3wQEUgiQ4yuvQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaQRR-66dFFu-983405219d8220b1
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's a good daily warm-up routine for desk workers? Keep it under 8 steps.' took 3.18s with response: 
LLMResponse(type='response', content='Here’s a simple warm-up routine for desk workers:\n\n1. Neck stretch: Gently tilt your head to the side, ear towards shoulder, and hold for 15 seconds. Repeat on the other side.\n2. Shoulder rolls: Roll your shoulders forward and backward in a circular motion for 15 seconds.\n3. Arm swing: Swing your arms in a forward and backward motion for 30 seconds.\n4. Wrist rotations: Rotate your wrists in a circular motion, 10 times clockwise and 10 times counterclockwise.\n5. Seated leg raise: Lift one leg and hold for 5 seconds, then lower. Repeat with the other leg.\n6. Hip circles: Sit upright and make small circles with your hips, first clockwise and then counterclockwise, for 15 seconds.\n7. Back stretch: Lean back in your chair and gently press your hands against the backrest, stretching your back. Hold for 15 seconds.\n8. Eye rest: Look away from your screen and focus on a distant object for 10 seconds to reduce eye strain.\n\nTry to perform these exercises every hour or so to keep your body active and reduce the risk of discomfort and injury.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d98bbe17-ad8f-44d1-bad2-639b3564fad7', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "I keep forgetting people's names—share 4 memory techniques."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0D5A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0D3C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:12 GMT'), (b'x-ratelimit-remaining-tokens', b'32550'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaSt3-66dFFu-983405549c84d6cf'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983405549c84d6cf-IAD'), (b'etag', b'W/"490-fEgd/PkvL5FsgStT6W2H0xCP0IQ"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:12.378Z'), (b'x-api-call-start', b'2025-09-22T18:56:11.174Z'), (b'x-api-received', b'2025-09-22T18:56:11.164Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e5b482ce8b5bb64cfe4de1d81504c0b6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b's4UDI5iUh0gX1k7IHJrIJCYeeiZgE4Bng50h9RBUfzf2EzFIto7hvQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:12 GMT', 'x-ratelimit-remaining-tokens': '32550', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaSt3-66dFFu-983405549c84d6cf', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983405549c84d6cf-IAD', 'etag': 'W/"490-fEgd/PkvL5FsgStT6W2H0xCP0IQ"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:12.378Z', 'x-api-call-start': '2025-09-22T18:56:11.174Z', 'x-api-received': '2025-09-22T18:56:11.164Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e5b482ce8b5bb64cfe4de1d81504c0b6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 's4UDI5iUh0gX1k7IHJrIJCYeeiZgE4Bng50h9RBUfzf2EzFIto7hvQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaSt3-66dFFu-983405549c84d6cf
DEBUG    test_performance:test_performance.py:279 Normal conversation 'I keep forgetting people's names—share 4 memory techniques.' took 1.67s with response: 
LLMResponse(type='response', content='Here are four memory techniques to help you remember people\'s names:\n\n1. **Repetition**: Repeat the name immediately after you hear it. For example, say, "Nice to meet you, John."\n2. **Association**: Create a mental image that links the name to something familiar. If you meet someone named "Oakley," imagine them holding a large oak leaf.\n3. **Acronyms**: Turn the name into an acronym. For "Elizabeth," you could use "E for elegant, L for lovely, I for intelligent, Z for zestful."\n4. **Repetition in Conversation**: Use the name in conversation to reinforce it. Ask questions like, "So, how long have you known Elizabeth?"', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-aba938d6-b3eb-41bb-aa03-db79f02d5aa6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Turn this into a bullet list: 'Plan, execute, measure, iterate.' Add one emoji each."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0F310>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0EEF0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:18 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaUqa-66dFFu-9834057e8ac8c095'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834057e8ac8c095-IAD'), (b'etag', b'W/"2d3-A250u2ADSh9/AKoOBc+TMl3RZPs"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:18.185Z'), (b'x-api-call-start', b'2025-09-22T18:56:17.759Z'), (b'x-api-received', b'2025-09-22T18:56:17.749Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 d2e1cc2538095700454cd55cac87c3bc.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'3tMUhQ43DikHCe6NaiQa5oKCSxVblz2wTUldClq58T1uW6xg7sJkeA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:18 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaUqa-66dFFu-9834057e8ac8c095', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834057e8ac8c095-IAD', 'etag': 'W/"2d3-A250u2ADSh9/AKoOBc+TMl3RZPs"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:18.185Z', 'x-api-call-start': '2025-09-22T18:56:17.759Z', 'x-api-received': '2025-09-22T18:56:17.749Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 d2e1cc2538095700454cd55cac87c3bc.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '3tMUhQ43DikHCe6NaiQa5oKCSxVblz2wTUldClq58T1uW6xg7sJkeA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaUqa-66dFFu-9834057e8ac8c095
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Turn this into a bullet list: 'Plan, execute, measure, iterate.' Add one emoji each.' took 0.89s with response: 
LLMResponse(type='function_call', content=None, function='format_text', module='text_manipulation', arguments={'input': 'Plan, execute, measure, iterate.', 'format': 'bullet_list', 'add_emoji': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7835dbf1-897e-4efb-8735-1de932b22283', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Write a tiny story (≤120 words) about a lighthouse learning Morse code.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F0FA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F1600>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:24 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaWcF-62bZhn-983405a38a595716'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983405a38a595716-IAD'), (b'etag', b'W/"355-OYbDfCrBAWwMvg0hB+nltaihVlM"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:24.360Z'), (b'x-api-call-start', b'2025-09-22T18:56:23.713Z'), (b'x-api-received', b'2025-09-22T18:56:23.705Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 164d117783fd653edf9d918ff78aa7e0.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'9rfjIAhCPM6gkFWJidObMRB7ocInqr8FevtZc7pk8KMMHV0ZWxHdyQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:24 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaWcF-62bZhn-983405a38a595716', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983405a38a595716-IAD', 'etag': 'W/"355-OYbDfCrBAWwMvg0hB+nltaihVlM"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:24.360Z', 'x-api-call-start': '2025-09-22T18:56:23.713Z', 'x-api-received': '2025-09-22T18:56:23.705Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 164d117783fd653edf9d918ff78aa7e0.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '9rfjIAhCPM6gkFWJidObMRB7ocInqr8FevtZc7pk8KMMHV0ZWxHdyQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaWcF-62bZhn-983405a38a595716
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Write a tiny story (≤120 words) about a lighthouse learning Morse code.' took 1.19s with response: 
LLMResponse(type='function_call', content=None, function='generate_story', module='story', arguments={'title': 'The Lighthouse Learns Morse', 'content': "In a cozy cove, the old lighthouse watched the sea. One night, a ship's light danced in its beam. The lighthouse blinked back, learning Morse code: 'All is well.'"})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-68fd02ea-6274-47db-9345-3bce03302bb7', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Explain CAP theorem like I'm new to distributed systems."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0F7F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0CD30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:31 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaYVW-3NKUce-983405ca384a7f1b'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983405ca384a7f1b-IAD'), (b'etag', b'W/"5df-XeEidIdKK2JBo2i47AsJRBvi0Gs"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:31.450Z'), (b'x-api-call-start', b'2025-09-22T18:56:30.054Z'), (b'x-api-received', b'2025-09-22T18:56:30.042Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 912fe8d927c37ab06a5ab9f3fd8505be.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'1oW1olECBCruvkD1Uo836jKpeb0x_vB21pT6p6K5MZkElu4gbzreMQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:31 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaYVW-3NKUce-983405ca384a7f1b', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983405ca384a7f1b-IAD', 'etag': 'W/"5df-XeEidIdKK2JBo2i47AsJRBvi0Gs"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:31.450Z', 'x-api-call-start': '2025-09-22T18:56:30.054Z', 'x-api-received': '2025-09-22T18:56:30.042Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 912fe8d927c37ab06a5ab9f3fd8505be.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '1oW1olECBCruvkD1Uo836jKpeb0x_vB21pT6p6K5MZkElu4gbzreMQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaYVW-3NKUce-983405ca384a7f1b
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Explain CAP theorem like I'm new to distributed systems.' took 1.81s with response: 
LLMResponse(type='response', content="The CAP theorem is a principle in distributed computing that states a distributed system can only guarantee at most two out of the following three consistency properties:\n\n- **C (Consistency)**: Every read receives the most recent write or an error.\n- **A (Availability)**: Every request receives a response about whether it succeeded or failed; the response cannot be delayed indefinitely.\n- **P (Partition Tolerance)**: The system continues to operate despite arbitrary message loss or failure of part of the system.\n\nIn simpler terms:\n- **Consistency** ensures that all nodes see the same data at the same time.\n- **Availability** ensures that the system is always operational and responsive.\n- **Partition Tolerance** means the system can handle network partitions (nodes can't communicate with each other) without failing.\n\nSince these three properties are often in conflict, a distributed system must choose which two to prioritize based on its requirements.", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-668aa7c9-9e8a-441f-86f2-982a232a940b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Compare SQLite vs PostgreSQL for a solo desktop app.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:37 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaaSB-66dFFu-983405f43b5f39b5'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983405f43b5f39b5-IAD'), (b'etag', b'W/"59f-3r43Xy4f9Y+P0bpsMnvgtFWyask"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:37.960Z'), (b'x-api-call-start', b'2025-09-22T18:56:36.586Z'), (b'x-api-received', b'2025-09-22T18:56:36.577Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 912fe8d927c37ab06a5ab9f3fd8505be.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'q3cpubc6c2wdPB2dvmSdipm_0z7XyyxgzaJiEE3TIOlGmL7TBjDRsA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:37 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaaSB-66dFFu-983405f43b5f39b5', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983405f43b5f39b5-IAD', 'etag': 'W/"59f-3r43Xy4f9Y+P0bpsMnvgtFWyask"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:37.960Z', 'x-api-call-start': '2025-09-22T18:56:36.586Z', 'x-api-received': '2025-09-22T18:56:36.577Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 912fe8d927c37ab06a5ab9f3fd8505be.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'q3cpubc6c2wdPB2dvmSdipm_0z7XyyxgzaJiEE3TIOlGmL7TBjDRsA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaaSB-66dFFu-983405f43b5f39b5
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Compare SQLite vs PostgreSQL for a solo desktop app.' took 1.66s with response: 
LLMResponse(type='response', content='SQLite and PostgreSQL are both relational database management systems, but they serve different needs and have different characteristics. SQLite is a lightweight, file-based database that is easy to use and does not require a separate server process. It is ideal for small to medium applications where the database does not need to be accessed by multiple users simultaneously. PostgreSQL, on the other hand, is a powerful, open-source object-relational database system that supports many SQL features and is known for its reliability, data integrity, and wide range of advanced features. It is better suited for larger applications or those that require concurrent access from multiple users. For a solo desktop app, SQLite might be more than sufficient due to its simplicity and ease of use. However, if you anticipate needing more advanced features or concurrent access, PostgreSQL could be a better choice.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c18159cd-973c-4655-a170-d4d4cead1552', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'How would you unit test a function that parses CSV lines? Keep it high level.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E219FAEF0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E219FAAD0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:44 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHacTN-62bZhn-9834061eb99ddddf'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834061eb99ddddf-IAD'), (b'etag', b'W/"46f-pZ8pR4vB3Z6Yqi9qCONA8EAOmK0"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:44.351Z'), (b'x-api-call-start', b'2025-09-22T18:56:43.382Z'), (b'x-api-received', b'2025-09-22T18:56:43.373Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7db525476c192850b65097a6bb612976.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'4h8RR4TZrh_vqZ8DAEuwyymhYvbz4oKdFcEwL-j7Q5TnSw7_jfcDwA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:44 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHacTN-62bZhn-9834061eb99ddddf', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834061eb99ddddf-IAD', 'etag': 'W/"46f-pZ8pR4vB3Z6Yqi9qCONA8EAOmK0"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:44.351Z', 'x-api-call-start': '2025-09-22T18:56:43.382Z', 'x-api-received': '2025-09-22T18:56:43.373Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7db525476c192850b65097a6bb612976.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '4h8RR4TZrh_vqZ8DAEuwyymhYvbz4oKdFcEwL-j7Q5TnSw7_jfcDwA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHacTN-62bZhn-9834061eb99ddddf
DEBUG    test_performance:test_performance.py:279 Normal conversation 'How would you unit test a function that parses CSV lines? Keep it high level.' took 1.52s with response: 
LLMResponse(type='response', content='To unit test a function that parses CSV lines, you would typically follow these high-level steps:\n\n1. Define test cases that cover various scenarios, including normal data, edge cases, and error conditions.\n2. Use a testing framework like PyTest or JUnit to write test functions.\n3. Write assertions to check if the output of the function matches the expected results.\n\nHere’s an example using Python and PyTest:\n\n{\n  "module": "unittest",\n  "function": "test",\n  "arguments": { "test_function": "test_parse_csv", "class_name": "CSVParserTest", "module": "csv_parser" }\n}', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b89f1cc5-88ed-4d6b-834c-c4b285b0f123', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the minimum I need to know about UX heuristics to not mess up my UI?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0CCA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0C880>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:50 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaePa-66dFFu-983406475d48aa75'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983406475d48aa75-IAD'), (b'etag', b'W/"2ee-2sYEo5yWy6XmX6HuqCB3AkWIO5k"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:50.378Z'), (b'x-api-call-start', b'2025-09-22T18:56:49.893Z'), (b'x-api-received', b'2025-09-22T18:56:49.882Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7db525476c192850b65097a6bb612976.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'hwpkhQiPDzRwD7bIyJ6AohCkT3LK8X1oW61ex7MVHv3kRN_N5T2kvQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:50 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaePa-66dFFu-983406475d48aa75', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983406475d48aa75-IAD', 'etag': 'W/"2ee-2sYEo5yWy6XmX6HuqCB3AkWIO5k"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:50.378Z', 'x-api-call-start': '2025-09-22T18:56:49.893Z', 'x-api-received': '2025-09-22T18:56:49.882Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7db525476c192850b65097a6bb612976.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'hwpkhQiPDzRwD7bIyJ6AohCkT3LK8X1oW61ex7MVHv3kRN_N5T2kvQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaePa-66dFFu-983406475d48aa75
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's the minimum I need to know about UX heuristics to not mess up my UI?' took 0.89s with response: 
LLMResponse(type='response', content='To effectively avoid common UI mistakes, you should understand basic UX heuristics such as consistency, simplicity, and usability. Ensure your design is intuitive and follows established patterns to enhance user experience.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e371675d-8ff6-40d5-9016-7de12d04d982', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "I'm anxious before presentations—give me 5 quick tips."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20A0D2A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E20993EE0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:56:56 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHagCx-3NKUce-9834066c5e50c94c'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834066c5e50c94c-IAD'), (b'etag', b'W/"38c-HH+rf8Fks/odXSynIzb8J4iKVQk"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:56:56.855Z'), (b'x-api-call-start', b'2025-09-22T18:56:56.004Z'), (b'x-api-received', b'2025-09-22T18:56:55.993Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 3dc3dc42adbf32dc7d24710c2307cb20.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'rrfRe3VAJxmRKDeHTGjFwmdHNXXHdiZdLJxaM3SIOApkv7XfEyGoCQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:56:56 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHagCx-3NKUce-9834066c5e50c94c', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834066c5e50c94c-IAD', 'etag': 'W/"38c-HH+rf8Fks/odXSynIzb8J4iKVQk"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:56:56.855Z', 'x-api-call-start': '2025-09-22T18:56:56.004Z', 'x-api-received': '2025-09-22T18:56:55.993Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 3dc3dc42adbf32dc7d24710c2307cb20.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'rrfRe3VAJxmRKDeHTGjFwmdHNXXHdiZdLJxaM3SIOApkv7XfEyGoCQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHagCx-3NKUce-9834066c5e50c94c
DEBUG    test_performance:test_performance.py:279 Normal conversation 'I'm anxious before presentations—give me 5 quick tips.' took 1.26s with response: 
LLMResponse(type='response', content='Sure, here are some quick tips to help you feel more confident before presentations:\n\n1. Prepare thoroughly by practicing your speech multiple times.\n2. Focus on your breathing to stay calm and composed.\n3. Engage with your audience to build a connection.\n4. Use positive self-talk to boost your confidence.\n5. Visualize a successful presentation to enhance your performance.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-80599387-6005-46e3-b0c8-1b1ed6c0e375', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Rewrite 'optimize the pipeline' in plainer language."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F1960>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E209F0E50>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:02 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHai1V-62bZhn-9834069369bbc987'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834069369bbc987-IAD'), (b'etag', b'W/"222-t1CHVmb5jR5LjDfw+Rijmr1DwNg"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:02.298Z'), (b'x-api-call-start', b'2025-09-22T18:57:02.064Z'), (b'x-api-received', b'2025-09-22T18:57:02.056Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 3b4be9f7cd333273a9f136c30c72d1f8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'WG4OH24ZFxRRSz2YST_Yr7nx5Gu1-bqAMwHwlcdJDbRHMhNub6nD4w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:02 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHai1V-62bZhn-9834069369bbc987', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834069369bbc987-IAD', 'etag': 'W/"222-t1CHVmb5jR5LjDfw+Rijmr1DwNg"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:02.298Z', 'x-api-call-start': '2025-09-22T18:57:02.064Z', 'x-api-received': '2025-09-22T18:57:02.056Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 3b4be9f7cd333273a9f136c30c72d1f8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'WG4OH24ZFxRRSz2YST_Yr7nx5Gu1-bqAMwHwlcdJDbRHMhNub6nD4w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHai1V-62bZhn-9834069369bbc987
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Rewrite 'optimize the pipeline' in plainer language.' took 0.73s with response: 
LLMResponse(type='response', content='improve the pipeline', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d64a0219-ad24-49f3-b39b-01fa834709d0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Brainstorm 6 team-building activities for remote developers.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0E470>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0E050>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:08 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHajkt-3NKUce-983406b739033898'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983406b739033898-IAD'), (b'etag', b'W/"2d3-WXmYx38ZQ2J/8sbZap9f6Plc88I"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:08.326Z'), (b'x-api-call-start', b'2025-09-22T18:57:07.947Z'), (b'x-api-received', b'2025-09-22T18:57:07.937Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'Y_sx7fASevbtX7we1mGjpDUQ9IrsZcIZQEZVk8BgKb7yefWkmj-2og==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:08 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHajkt-3NKUce-983406b739033898', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983406b739033898-IAD', 'etag': 'W/"2d3-WXmYx38ZQ2J/8sbZap9f6Plc88I"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:08.326Z', 'x-api-call-start': '2025-09-22T18:57:07.947Z', 'x-api-received': '2025-09-22T18:57:07.937Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'Y_sx7fASevbtX7we1mGjpDUQ9IrsZcIZQEZVk8BgKb7yefWkmj-2og=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHajkt-3NKUce-983406b739033898
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Brainstorm 6 team-building activities for remote developers.' took 0.84s with response: 
LLMResponse(type='function_call', content=None, function='generate_team_building_activities', module='brainstorming', arguments={'team_type': 'remote developers', 'number_of_activities': 6})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7f78db22-8bdc-448e-a7f4-c2f7133a8806', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the trade-off between early abstraction and duplication?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:14 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHamSL-66dFFu-983406db2b1e54bb'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983406db2b1e54bb-IAD'), (b'etag', b'W/"35c-e3vkoezq3mjRyUdzF6nNwm8ZiHE"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:14.168Z'), (b'x-api-call-start', b'2025-09-22T18:57:13.602Z'), (b'x-api-received', b'2025-09-22T18:57:13.590Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'xQW1fO7D_KgqlpmarbG-nHGfE68Sbr0qYVd_xB03FoHFohOmoiw0ww==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:14 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHamSL-66dFFu-983406db2b1e54bb', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983406db2b1e54bb-IAD', 'etag': 'W/"35c-e3vkoezq3mjRyUdzF6nNwm8ZiHE"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:14.168Z', 'x-api-call-start': '2025-09-22T18:57:13.602Z', 'x-api-received': '2025-09-22T18:57:13.590Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'xQW1fO7D_KgqlpmarbG-nHGfE68Sbr0qYVd_xB03FoHFohOmoiw0ww=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHamSL-66dFFu-983406db2b1e54bb
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's the trade-off between early abstraction and duplication?' took 1.02s with response: 
LLMResponse(type='response', content='Early abstraction can help in reducing code duplication by promoting reusable components, but it might also introduce complexity and make the code harder to understand initially. Duplication, on the other hand, keeps things simple and easy to understand but can lead to maintenance issues if the duplicated code needs to be updated.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-86bb9c12-aee1-4f79-88a0-96617eb4eec4', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Explain why floating-point math can be surprising to newcomers.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:20 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaoE1-66dFFu-98340700ecdce602'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'98340700ecdce602-IAD'), (b'etag', b'W/"44f-3MfEagfWP13ggCDlW2W3mgGEOPw"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:20.558Z'), (b'x-api-call-start', b'2025-09-22T18:57:19.611Z'), (b'x-api-received', b'2025-09-22T18:57:19.602Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'5fWvDda5Rt_YQIf5-NvldUNI9Vz5sk-_zF_zQEU22cELC836lI_AQw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:20 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaoE1-66dFFu-98340700ecdce602', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '98340700ecdce602-IAD', 'etag': 'W/"44f-3MfEagfWP13ggCDlW2W3mgGEOPw"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:20.558Z', 'x-api-call-start': '2025-09-22T18:57:19.611Z', 'x-api-received': '2025-09-22T18:57:19.602Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '5fWvDda5Rt_YQIf5-NvldUNI9Vz5sk-_zF_zQEU22cELC836lI_AQw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaoE1-66dFFu-98340700ecdce602
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Explain why floating-point math can be surprising to newcomers.' took 1.26s with response: 
LLMResponse(type='response', content='Floating-point arithmetic can be surprising to newcomers because the way computers represent and process decimal numbers can lead to small inaccuracies. This is due to the fact that floating-point numbers are stored in a binary format, which can sometimes not precisely represent decimal fractions. For example, the fraction 1/10 cannot be accurately represented in binary, leading to small rounding errors when performing arithmetic operations. This can result in unexpected outcomes when comparing floating-point numbers or expecting exact decimal results from calculations.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-25ca1012-a9de-4f11-8af2-862b78d2a3c0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Give me a one-liner pep talk for debugging at 2 a.m.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:26 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHaq7e-3NKUce-98340727fa5cd694'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'98340727fa5cd694-IAD'), (b'etag', b'W/"26c-ungGqCCiHbUhXpkHGpG87HUDOcQ"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:26.271Z'), (b'x-api-call-start', b'2025-09-22T18:57:25.984Z'), (b'x-api-received', b'2025-09-22T18:57:25.962Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'quq-KT7WrClotkKq1LS_XYXJNaJ2L9hW14sIXjv02bBdaUEzuDnE-A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:26 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHaq7e-3NKUce-98340727fa5cd694', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '98340727fa5cd694-IAD', 'etag': 'W/"26c-ungGqCCiHbUhXpkHGpG87HUDOcQ"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:26.271Z', 'x-api-call-start': '2025-09-22T18:57:25.984Z', 'x-api-received': '2025-09-22T18:57:25.962Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'quq-KT7WrClotkKq1LS_XYXJNaJ2L9hW14sIXjv02bBdaUEzuDnE-A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHaq7e-3NKUce-98340727fa5cd694
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Give me a one-liner pep talk for debugging at 2 a.m.' took 0.56s with response: 
LLMResponse(type='response', content="Keep going, every line of code you check brings you closer to the solution. You've got this!", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ed846547-39c3-4ee4-951a-850bf29c5b9d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'List 5 git hygiene practices for small teams.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0C580>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21A0CA00>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:57:32 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHarkz-66dFFu-9834074b59719807'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834074b59719807-IAD'), (b'etag', b'W/"3b8-/XCV85qS/6AwEruTjwcYtahzSKg"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:57:32.170Z'), (b'x-api-call-start', b'2025-09-22T18:57:31.502Z'), (b'x-api-received', b'2025-09-22T18:57:31.491Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e5b482ce8b5bb64cfe4de1d81504c0b6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'DRcOvAsxydFDB8pTXstnXrzmVPoPQL3kC4erqyGwrbZkw-NgLDq-6g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:57:32 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHarkz-66dFFu-9834074b59719807', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834074b59719807-IAD', 'etag': 'W/"3b8-/XCV85qS/6AwEruTjwcYtahzSKg"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:57:32.170Z', 'x-api-call-start': '2025-09-22T18:57:31.502Z', 'x-api-received': '2025-09-22T18:57:31.491Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e5b482ce8b5bb64cfe4de1d81504c0b6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'DRcOvAsxydFDB8pTXstnXrzmVPoPQL3kC4erqyGwrbZkw-NgLDq-6g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHarkz-66dFFu-9834074b59719807
DEBUG    test_performance:test_performance.py:279 Normal conversation 'List 5 git hygiene practices for small teams.' took 1.25s with response: 
LLMResponse(type='response', content='Here are five git hygiene practices for small teams:\n\n1. Regularly merge changes from the main branch into feature branches to keep them up-to-date.\n2. Use meaningful commit messages that describe the changes made.\n3. Review pull requests before merging to ensure code quality and functionality.\n4. Set up automated tests to catch issues early.\n5. Use branches for features and bug fixes to keep the main branch stable.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a20ac23e-94e5-40c0-b456-dfdba8249cf0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's a simple analogy for gradient descent?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F3CA043D00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F3C873F840> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F3CA043A30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 21:41:08 GMT'), (b'x-ratelimit-remaining-tokens', b'25609'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCJTAdD-3NKUce-9834f6f04e0e0739'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834f6f04e0e0739-IAD'), (b'etag', b'W/"3e0-gft9I8oySPF6naipiT9qeH1a3+0"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T21:41:08.117Z'), (b'x-api-call-start', b'2025-09-22T21:41:07.202Z'), (b'x-api-received', b'2025-09-22T21:41:07.192Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MAD53-P3'), (b'X-Amz-Cf-Id', b'_C3_5K4FcyNLlrVwJV36zcLnV9zouLZkMuAxzZGvhCJONxRFQnJidw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 21:41:08 GMT', 'x-ratelimit-remaining-tokens': '25609', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCJTAdD-3NKUce-9834f6f04e0e0739', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834f6f04e0e0739-IAD', 'etag': 'W/"3e0-gft9I8oySPF6naipiT9qeH1a3+0"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T21:41:08.117Z', 'x-api-call-start': '2025-09-22T21:41:07.202Z', 'x-api-received': '2025-09-22T21:41:07.192Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MAD53-P3', 'x-amz-cf-id': '_C3_5K4FcyNLlrVwJV36zcLnV9zouLZkMuAxzZGvhCJONxRFQnJidw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCJTAdD-3NKUce-9834f6f04e0e0739
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's a simple analogy for gradient descent?' took 2.04s with response: 
LLMResponse(type='response', content='Imagine you\'re trying to find the lowest point in a hilly landscape. You start at a random spot and look around to find the steepest direction downhill. You take a step in that direction and repeat the process, always moving towards the steepest downhill slope until you reach a point where you can\'t go lower anymore. That\'s roughly what gradient descent does, but in the context of a mathematical function where "downhill" is the direction of steepest descent.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b0af8f05-1266-418e-94ac-02a8b72a6bfe', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Suggest 4 interview questions to test problem decomposition skills.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 21:41:14 GMT'), (b'x-ratelimit-remaining-tokens', b'26477'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCJTCXR-62bZhn-9834f716fd469c8a'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834f716fd469c8a-IAD'), (b'etag', b'W/"2b4-3rXoFK0cBiimfQEjoJPRlzov/J0"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T21:41:13.997Z'), (b'x-api-call-start', b'2025-09-22T21:41:13.593Z'), (b'x-api-received', b'2025-09-22T21:41:13.584Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MAD53-P3'), (b'X-Amz-Cf-Id', b'JZdFiCh98S7cYVhHleZJbIKhtvDR-VS4umAhe9hgCM6ce8m7s-8Q4g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 21:41:14 GMT', 'x-ratelimit-remaining-tokens': '26477', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCJTCXR-62bZhn-9834f716fd469c8a', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834f716fd469c8a-IAD', 'etag': 'W/"2b4-3rXoFK0cBiimfQEjoJPRlzov/J0"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T21:41:13.997Z', 'x-api-call-start': '2025-09-22T21:41:13.593Z', 'x-api-received': '2025-09-22T21:41:13.584Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MAD53-P3', 'x-amz-cf-id': 'JZdFiCh98S7cYVhHleZJbIKhtvDR-VS4umAhe9hgCM6ce8m7s-8Q4g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCJTCXR-62bZhn-9834f716fd469c8a
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Suggest 4 interview questions to test problem decomposition skills.' took 0.75s with response: 
LLMResponse(type='function_call', content=None, function='suggest_interview_questions', module='hr', arguments={'skill': 'problem decomposition', 'number_of_questions': 4})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-48a633fb-a01d-48bd-b965-81367ff83f96', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'How do I politely push back on scope creep?'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 21:41:19 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCJTDym-3NKUce-9834f73afd4107fb'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834f73afd4107fb-IAD'), (b'etag', b'W/"2e5-0mf/n+APYI37J5kiEm82sj0fL94"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T21:41:18.986Z'), (b'x-api-call-start', b'2025-09-22T21:41:18.500Z'), (b'x-api-received', b'2025-09-22T21:41:18.478Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MAD53-P3'), (b'X-Amz-Cf-Id', b'Ny-WTh2dYQ42mvUwQNo4EHZhWhQT7tKD9osKPl2cg6ghJvaBg5PPQQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 21:41:19 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCJTDym-3NKUce-9834f73afd4107fb', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834f73afd4107fb-IAD', 'etag': 'W/"2e5-0mf/n+APYI37J5kiEm82sj0fL94"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T21:41:18.986Z', 'x-api-call-start': '2025-09-22T21:41:18.500Z', 'x-api-received': '2025-09-22T21:41:18.478Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MAD53-P3', 'x-amz-cf-id': 'Ny-WTh2dYQ42mvUwQNo4EHZhWhQT7tKD9osKPl2cg6ghJvaBg5PPQQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCJTDym-3NKUce-9834f73afd4107fb
DEBUG    test_performance:test_performance.py:279 Normal conversation 'How do I politely push back on scope creep?' took 0.79s with response: 
LLMResponse(type='response', content='To politely push back on scope creep, you might say, "I understand the desire to expand the project, but let\'s review the original scope and priorities to ensure we\'re still aligned with our goals and resources."', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e6252741-d4c8-4697-b6b1-772bdc5473ce', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Convert this to title case: 'an introduction to concurrency primitives'"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 21:41:25 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCJTFi2-3NKUce-9834f75f2c7c07fb'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834f75f2c7c07fb-IAD'), (b'etag', b'W/"2a7-vAoCZSa7iMiRl8XF+7LktDD1jQo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T21:41:24.715Z'), (b'x-api-call-start', b'2025-09-22T21:41:24.301Z'), (b'x-api-received', b'2025-09-22T21:41:24.291Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MAD53-P3'), (b'X-Amz-Cf-Id', b'jsWonlIAn3JHXxOdj13xFVF7I85EHENkFvBoa8RrpFutjYG5DAak3A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 21:41:25 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCJTFi2-3NKUce-9834f75f2c7c07fb', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834f75f2c7c07fb-IAD', 'etag': 'W/"2a7-vAoCZSa7iMiRl8XF+7LktDD1jQo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T21:41:24.715Z', 'x-api-call-start': '2025-09-22T21:41:24.301Z', 'x-api-received': '2025-09-22T21:41:24.291Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2afdb7947d4ee2ad8275c94857985058.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MAD53-P3', 'x-amz-cf-id': 'jsWonlIAn3JHXxOdj13xFVF7I85EHENkFvBoa8RrpFutjYG5DAak3A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCJTFi2-3NKUce-9834f75f2c7c07fb
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Convert this to title case: 'an introduction to concurrency primitives'' took 0.78s with response: 
LLMResponse(type='function_call', content=None, function='convert_to_title_case', module='text', arguments={'text': 'an introduction to concurrency primitives'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b5496c61-c540-4020-9fcd-60d98f70f204', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Name 5 signs a backlog item isn't ready for dev."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F3CA0E29B0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F3C873F840> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F3CA0E2590>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 21:41:31 GMT'), (b'x-ratelimit-remaining-tokens', b'29292'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCJTHgE-66dFFu-9834f783b8a0241e'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834f783b8a0241e-IAD'), (b'etag', b'W/"2d0-w5NPYTbIiE1/gOUCqLsswO/R4RU"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T21:41:31.325Z'), (b'x-api-call-start', b'2025-09-22T21:41:30.927Z'), (b'x-api-received', b'2025-09-22T21:41:30.916Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 d492c5a0060c27619b387cdebf7bb28a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MAD53-P3'), (b'X-Amz-Cf-Id', b'ql8NpdnvoUPTp8yDntwW14fLcs7uMmb10V0YCJu3osmXNsbrrjQuZA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 21:41:31 GMT', 'x-ratelimit-remaining-tokens': '29292', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCJTHgE-66dFFu-9834f783b8a0241e', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834f783b8a0241e-IAD', 'etag': 'W/"2d0-w5NPYTbIiE1/gOUCqLsswO/R4RU"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T21:41:31.325Z', 'x-api-call-start': '2025-09-22T21:41:30.927Z', 'x-api-received': '2025-09-22T21:41:30.916Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 d492c5a0060c27619b387cdebf7bb28a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MAD53-P3', 'x-amz-cf-id': 'ql8NpdnvoUPTp8yDntwW14fLcs7uMmb10V0YCJu3osmXNsbrrjQuZA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCJTHgE-66dFFu-9834f783b8a0241e
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Name 5 signs a backlog item isn't ready for dev.' took 0.74s with response: 
LLMResponse(type='response', content="When a backlog item isn't ready for development, it might lack the following:\n\n1. Clear description\n2. User story format\n3. Acceptance criteria\n4. Estimated effort\n5. Priority or due date", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9892f823-1fde-42f6-a627-73d56ed78b91', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's a good rubric to decide whether to refactor now or later?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB1240>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB0E20>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:58:06 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHb2Ue-66dFFu-983408178a6c3725'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983408178a6c3725-IAD'), (b'etag', b'W/"645-Dn7Q8z9i57uhfNa2LcuCE9oYU8E"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:58:06.309Z'), (b'x-api-call-start', b'2025-09-22T18:58:04.193Z'), (b'x-api-received', b'2025-09-22T18:58:04.184Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'XfA-clGfA6affdD83HApI2m9ogMKEMZEHcSvngxR5Lxsrddo48dEHw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:58:06 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHb2Ue-66dFFu-983408178a6c3725', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983408178a6c3725-IAD', 'etag': 'W/"645-Dn7Q8z9i57uhfNa2LcuCE9oYU8E"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:58:06.309Z', 'x-api-call-start': '2025-09-22T18:58:04.193Z', 'x-api-received': '2025-09-22T18:58:04.184Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9874b25ddff24dd18d4122c6a190ace4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'XfA-clGfA6affdD83HApI2m9ogMKEMZEHcSvngxR5Lxsrddo48dEHw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHb2Ue-66dFFu-983408178a6c3725
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's a good rubric to decide whether to refactor now or later?' took 2.54s with response: 
LLMResponse(type='response', content='Deciding whether to refactor code now or later can be approached using a few key criteria. Here’s a simple rubric you can use:\n\n1. **Criticality of the Feature**: If the feature is critical and the refactor will significantly improve its stability or maintainability, it might be worth refactoring now.\n2. **Complexity of the Code**: If the code is complex and hard to understand, refactoring can help improve readability and maintainability, making it a good candidate for immediate refactoring.\n3. **Risk of Technical Debt**: If the code is accumulating technical debt and not being refactored, it could lead to more issues in the future, making it a priority.\n4. **Time Constraints**: If the feature is not time-sensitive and you have the bandwidth, it might be better to refactor now to save time in the long run.\n5. **Future Changes**: If you foresee that the feature will need to change frequently in the near future, refactoring now can make future changes easier and less error-prone.\n\nIf none of these criteria are met, it might be better to refactor later.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b8128eee-d367-44aa-9c13-06841c8e3d8b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Give me a tiny regex cheat sheet (anchors, groups, classes).'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB2FB0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB2B90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:58:12 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHb4ef-3NKUce-98340846cdffc130'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'98340846cdffc130-IAD'), (b'etag', b'W/"3fe-yhoSrStMSxe5wlAQU6paZbxJm0A"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:58:12.527Z'), (b'x-api-call-start', b'2025-09-22T18:58:11.510Z'), (b'x-api-received', b'2025-09-22T18:58:11.494Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 6f2c3ba225fa958f54f027c65bb03d92.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'5-tKWpgYX9rG_KOR_oUIIgE7BauzHR9tXzxsWUIfnN1AIAXjFOg8wg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:58:12 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHb4ef-3NKUce-98340846cdffc130', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '98340846cdffc130-IAD', 'etag': 'W/"3fe-yhoSrStMSxe5wlAQU6paZbxJm0A"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:58:12.527Z', 'x-api-call-start': '2025-09-22T18:58:11.510Z', 'x-api-received': '2025-09-22T18:58:11.494Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 6f2c3ba225fa958f54f027c65bb03d92.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '5-tKWpgYX9rG_KOR_oUIIgE7BauzHR9tXzxsWUIfnN1AIAXjFOg8wg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHb4ef-3NKUce-98340846cdffc130
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Give me a tiny regex cheat sheet (anchors, groups, classes).' took 1.52s with response: 
LLMResponse(type='response', content="Sure! Here's a tiny regex cheat sheet:\n\n- Anchors:\n  - `^` matches the start of a string\n  - `$` matches the end of a string\n\n- Groups:\n  - `()` creates a group\n  - `\\1`, `\\2`, etc., refer to the first, second, etc., group\n\n- Character Classes:\n  - `[abc]` matches any character in the set (e.g., `a` or `b` or `c`)\n  - `[^abc]` matches any character NOT in the set\n  - `[a-z]` matches any lowercase letter\n  - `[0-9]` matches any digit\n\nLet me know if you need more details!", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-38548193-dad4-4e66-bf11-8a6ee16b654b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the 80/20 of Docker I should know to ship a Python app?"}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB3FA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AB3B80>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:58:20 GMT'), (b'x-ratelimit-remaining-tokens', b'32579'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHb6ib-66dFFu-9834086f9eafc934'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'9834086f9eafc934-IAD'), (b'etag', b'W/"5bb-bSnw+8rSSIolw37I+vWut7fTIq8"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:58:20.699Z'), (b'x-api-call-start', b'2025-09-22T18:58:18.460Z'), (b'x-api-received', b'2025-09-22T18:58:18.449Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 6420b771591b2927aaad391c05ef7460.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'6GGAJ6x16ra3WMN0jFvNWSEVpViBK1NnucRX8FgiqJB5LJmXmJNjaQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:58:20 GMT', 'x-ratelimit-remaining-tokens': '32579', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHb6ib-66dFFu-9834086f9eafc934', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '9834086f9eafc934-IAD', 'etag': 'W/"5bb-bSnw+8rSSIolw37I+vWut7fTIq8"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:58:20.699Z', 'x-api-call-start': '2025-09-22T18:58:18.460Z', 'x-api-received': '2025-09-22T18:58:18.449Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 6420b771591b2927aaad391c05ef7460.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': '6GGAJ6x16ra3WMN0jFvNWSEVpViBK1NnucRX8FgiqJB5LJmXmJNjaQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHb6ib-66dFFu-9834086f9eafc934
DEBUG    test_performance:test_performance.py:279 Normal conversation 'What's the 80/20 of Docker I should know to ship a Python app?' took 2.98s with response: 
LLMResponse(type='response', content='To ship a Python app using Docker, you should know how to create a Dockerfile that includes your Python application and its dependencies, and how to build and run a Docker image. Here are the key points:\n\n1. **Dockerfile**: This file contains instructions on how to build your Docker image. It typically starts with a base image (like `python:3.9-slim`), installs necessary dependencies, copies your application code, and sets the command to run your application.\n\n2. **Dependencies**: Ensure all dependencies are installed in the Docker image. You can use `pip install -r requirements.txt` to install dependencies from a `requirements.txt` file.\n\n3. **Build the Docker Image**: Use the `docker build` command to build your Docker image from the Dockerfile.\n\n4. **Run the Docker Image**: Use the `docker run` command to start a container from your image.\n\nWould you like a sample Dockerfile or more details on any of these steps?', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7bc64ffb-3f6c-4c54-996b-6428f1877baa', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Explain the difference between correlation and causation with one example.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AC9D50>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21AC9930>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:58:27 GMT'), (b'x-ratelimit-remaining-tokens', b'32599'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHb932-66dFFu-983408a17def82e4'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983408a17def82e4-IAD'), (b'etag', b'W/"461-0gBdX7zRNyvBq8pdFxXGuJF/RhI"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:58:27.238Z'), (b'x-api-call-start', b'2025-09-22T18:58:26.255Z'), (b'x-api-received', b'2025-09-22T18:58:26.245Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f6d06657c3cf81ebd927805f386acb54.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'Of5sFxvhc04fiXpEg9mVH0knZ9o0aSl47gMSv104eO7Ib5FvGF38Eg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:58:27 GMT', 'x-ratelimit-remaining-tokens': '32599', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHb932-66dFFu-983408a17def82e4', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983408a17def82e4-IAD', 'etag': 'W/"461-0gBdX7zRNyvBq8pdFxXGuJF/RhI"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:58:27.238Z', 'x-api-call-start': '2025-09-22T18:58:26.255Z', 'x-api-received': '2025-09-22T18:58:26.245Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f6d06657c3cf81ebd927805f386acb54.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'Of5sFxvhc04fiXpEg9mVH0knZ9o0aSl47gMSv104eO7Ib5FvGF38Eg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHb932-66dFFu-983408a17def82e4
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Explain the difference between correlation and causation with one example.' took 1.44s with response: 
LLMResponse(type='response', content='Correlation means that two variables tend to change together, but it does not imply that one causes the other. For example, there might be a correlation between ice cream sales and drowning incidents, as both tend to increase in the summer months. However, this does not mean that ice cream sales cause drowning incidents. The actual cause is the warmer weather.\n\nCausation, on the other hand, implies that one event is the result of the occurrence of the other event; i.e., there is a causal relationship between the two events. For example, smoking causes an increased risk of lung cancer.', function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-addb3f92-ba0e-4816-94bc-58a487332367', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "Turn 'I will try' into 3 stronger alternatives."}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21ACBAC0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015E20023D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015E21ACB6A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Mon, 22 Sep 2025 18:58:32 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCHbAmK-3NKUce-983408c9ce94c56f'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983408c9ce94c56f-IAD'), (b'etag', b'W/"269-FUEP83Rujfw33rEBTVz2piPVunQ"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-22T18:58:32.351Z'), (b'x-api-call-start', b'2025-09-22T18:58:32.073Z'), (b'x-api-received', b'2025-09-22T18:58:32.063Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 0d7deb275c6269c1898f9fac0cd690b6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MRS52-P3'), (b'X-Amz-Cf-Id', b'JesyvmZEE_ak7PSqgLL5tit-v4uaduWjFgbBMhd_U9W2BbKWNCL0PQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Mon, 22 Sep 2025 18:58:32 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCHbAmK-3NKUce-983408c9ce94c56f', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983408c9ce94c56f-IAD', 'etag': 'W/"269-FUEP83Rujfw33rEBTVz2piPVunQ"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-22T18:58:32.351Z', 'x-api-call-start': '2025-09-22T18:58:32.073Z', 'x-api-received': '2025-09-22T18:58:32.063Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 0d7deb275c6269c1898f9fac0cd690b6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'MRS52-P3', 'x-amz-cf-id': 'JesyvmZEE_ak7PSqgLL5tit-v4uaduWjFgbBMhd_U9W2BbKWNCL0PQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCHbAmK-3NKUce-983408c9ce94c56f
DEBUG    test_performance:test_performance.py:279 Normal conversation 'Turn 'I will try' into 3 stronger alternatives.' took 0.66s with response: 
LLMResponse(type='response', content='["I will definitely try", "I will make sure to try", "I will give it my best shot"]', function=None, module=None, arguments=None)


