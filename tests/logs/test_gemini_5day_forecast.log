DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8d25f82a-71a4-4339-94df-67d7d97e7096', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast for 40.7128, -74.0060.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09203C70>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092039A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'P3P', b'CP="This is not a P3P policy! See g.co/p3phelp for more info."'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:09:35 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=684'), (b'Set-Cookie', b'NID=525=eD_MR7SSIIcgwY4sccB0PL_SjomWD3DExy2BMgGjWE_QLbMYkadaoWd5Dl3Byg2K_hyFdKgCAJ_ch9Y7HMGMXnUs1kHaxzewGME46luDyK5KgaJCaSE_q0cHESK80QYQjDaEgL3BHySsjLPi6rWSW_CopkPNzN4jFwOaMNSc3f-sNsmCbYRn86_D9iVkPPrADHonepkmBaYiPfDyKh9BUp3Y8g; expires=Tue, 24-Mar-2026 18:09:34 GMT; path=/; domain=.generativelanguage.googleapis.com; HttpOnly'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Expires', b'Mon, 22 Sep 2025 18:09:35 GMT'), (b'Cache-Control', b'private'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('p3p', 'CP="This is not a P3P policy! See g.co/p3phelp for more info."'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:09:35 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=684'), ('set-cookie', 'NID=525=eD_MR7SSIIcgwY4sccB0PL_SjomWD3DExy2BMgGjWE_QLbMYkadaoWd5Dl3Byg2K_hyFdKgCAJ_ch9Y7HMGMXnUs1kHaxzewGME46luDyK5KgaJCaSE_q0cHESK80QYQjDaEgL3BHySsjLPi6rWSW_CopkPNzN4jFwOaMNSc3f-sNsmCbYRn86_D9iVkPPrADHonepkmBaYiPfDyKh9BUp3Y8g; expires=Tue, 24-Mar-2026 18:09:34 GMT; path=/; domain=.generativelanguage.googleapis.com; HttpOnly'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('expires', 'Mon, 22 Sep 2025 18:09:35 GMT'), ('cache-control', 'private'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day forecast for 40.7128, -74.0060.' took 2.01s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 40.7128, 'lon': -74.006})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-30912053-f944-478f-aaff-f6b2a8c08be8', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days near -33.8688, 151.2093?'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09241840>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09240F10>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'P3P', b'CP="This is not a P3P policy! See g.co/p3phelp for more info."'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:09:41 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=926'), (b'Set-Cookie', b'NID=525=fPx8cI7q1o3QMTJhAslDRFXZQ3ZoaePyaYbesCQhkZzAdvPvbhBO5rZLfYjaC17vNXSASN6bn_hDt8yvdraHRXkeiocN2ubNFGIPOlwCKixl6VYz9dA02K6djd-Wa5no___aGvP0n98EdS9iU7Hc3Nv3iO1aFZBvtzpmx_3EAYfP9R_XOMaQkVFNzMz3zBo_aBDJYr31XlKlOhc2uqEdH0T3jQ; expires=Tue, 24-Mar-2026 18:09:34 GMT; path=/; domain=.generativelanguage.googleapis.com; HttpOnly'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Expires', b'Mon, 22 Sep 2025 18:09:41 GMT'), (b'Cache-Control', b'private'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('p3p', 'CP="This is not a P3P policy! See g.co/p3phelp for more info."'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:09:41 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=926'), ('set-cookie', 'NID=525=fPx8cI7q1o3QMTJhAslDRFXZQ3ZoaePyaYbesCQhkZzAdvPvbhBO5rZLfYjaC17vNXSASN6bn_hDt8yvdraHRXkeiocN2ubNFGIPOlwCKixl6VYz9dA02K6djd-Wa5no___aGvP0n98EdS9iU7Hc3Nv3iO1aFZBvtzpmx_3EAYfP9R_XOMaQkVFNzMz3zBo_aBDJYr31XlKlOhc2uqEdH0T3jQ; expires=Tue, 24-Mar-2026 18:09:34 GMT; path=/; domain=.generativelanguage.googleapis.com; HttpOnly'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('expires', 'Mon, 22 Sep 2025 18:09:41 GMT'), ('cache-control', 'private'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Next 5 days near -33.8688, 151.2093?' took 1.03s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -33.8688, 'lon': 151.2093})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-849bf34b-ba24-4efa-9788-5a91a2c1f09b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast (5d) at -23.5505, -46.6333.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09243340>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09242BC0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:09:47 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=672'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:09:47 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=672'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast (5d) at -23.5505, -46.6333.' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -23.5505, 'lon': -46.6333})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1683c5a5-8547-46d2-9e53-144233c49f93', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Show me 5-day outlook for 30.0444, 31.2357.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09241330>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09241A50>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:09:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=583'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:09:52 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=583'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Show me 5-day outlook for 30.0444, 31.2357.' took 0.77s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 30.0444, 'lon': 31.2357})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-de2f9713-2174-41d7-b34a-da089f7f1d51', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the week ahead at 55.7558, 37.6173?"}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092A9630>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092A8EB0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:09:58 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=879'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:09:58 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=879'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'What's the week ahead at 55.7558, 37.6173?' took 0.99s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 55.7558, 'lon': 37.6173})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-946d46fa-9dd4-4c20-a80c-35552caeb37b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day forecast around -1.2921, 36.8219.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092AAE90>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092AA710>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:05 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1280'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:05 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1280'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Five-day forecast around -1.2921, 36.8219.' took 1.43s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -1.2921, 'lon': 36.8219})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-abd346d6-c8bd-4abb-930f-f5259e89dab5', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Is it cooling later this week near 64.1466, -21.9426?'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092B8730>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092B8040>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:11 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=572'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:11 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=572'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Is it cooling later this week near 64.1466, -21.9426?' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 64.1466, 'lon': -21.9426})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-82b9dcb4-aada-465f-aaff-45e28581de5e', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Give 5-day hi/lo for -33.9249, 18.4241.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=724'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:16 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=724'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Give 5-day hi/lo for -33.9249, 18.4241.' took 0.84s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': -33.9249, 'lon': 18.4241})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1667bded-997a-40ac-85ce-62a287b43bc2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast next days at 35.6895, 139.6917.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:23 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1050'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:23 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1050'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast next days at 35.6895, 139.6917.' took 1.09s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 35.6895, 'lon': 139.6917})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ec3d2d24-0f86-42d2-bf0b-d54611f13a43', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day for 38.7223, -9.1393 (Lisbon).'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:28 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=809'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:28 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=809'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Five-day for 38.7223, -9.1393 (Lisbon).' took 0.98s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'lat': 38.7223, 'lon': -9.1393})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-20437424-5180-452c-a6dd-81664dca1f9c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast 10001, US.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092BA770>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092B9FF0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:34 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=771'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:34 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=771'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day forecast 10001, US.' took 0.93s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '10001', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-31181a3c-4799-4eff-9c4c-7bfbaf07e9ee', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast for 90210, US next 5 days.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:40 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=592'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:40 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=592'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast for 90210, US next 5 days.' took 0.64s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '90210', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f7735f3a-f2d2-4ded-92d7-d5599625c3d9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Weather outlook 60614, US.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B1480>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B0D00>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:46 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=749'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:46 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=749'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Weather outlook 60614, US.' took 0.91s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '60614', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1e3a09c0-00ab-4509-98ac-57fbbb9276b2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for 10115, DE.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09242620>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F09242650>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=679'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:52 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=679'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day for 10115, DE.' took 0.83s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '10115', 'country_code': 'DE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-083ba126-da11-41cf-b508-eb865e530ecc', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next five days SW1A 1AA, GB.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092B9F60>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092BAFB0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:10:58 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1400'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:10:58 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1400'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Next five days SW1A 1AA, GB.' took 1.54s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': 'SW1A 1AA', 'country_code': 'GB'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4d817c49-9e41-4fd2-bac5-9516fe414ded', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast 75001, FR.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092AA5F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F092A9300>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:04 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=696'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:04 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=696'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast 75001, FR.' took 0.80s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '75001', 'country_code': 'FR'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-99b759ec-7db8-4857-acb7-635e10679d5b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for 1250-096, PT.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B0760>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B0D90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:10 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=842'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:10 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=842'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day for 1250-096, PT.' took 1.06s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '1250-096', 'country_code': 'PT'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8c98cd37-ce88-4e24-949a-b0e8a060566c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast for 2000, AU.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2D8370>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B3BB0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=806'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:16 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=806'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast for 2000, AU.' took 1.04s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '2000', 'country_code': 'AU'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0cf5aa3a-47b3-453a-b7a0-597f5da8c259', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day outlook 01000-000, BR.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2D9C00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2D9480>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:23 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1035'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:23 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1035'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day outlook 01000-000, BR.' took 1.23s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '01000-000', 'country_code': 'BR'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-007305bb-5ace-4dfb-971f-553360ce6854', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days 110001, IN.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:29 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=977'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:29 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=977'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Next 5 days 110001, IN.' took 1.04s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'zip': '110001', 'country_code': 'IN'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1ada6716-c55d-4030-81fa-b0a46ec7c42f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Five-day forecast Kyoto, JP.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:34 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=704'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:34 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=704'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Five-day forecast Kyoto, JP.' took 0.84s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Kyoto', 'country_code': 'JP'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6d9e9828-6dfd-482c-8b82-362ef2eb0da1', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's the 5-day in Toronto, CA?"}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B2440>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B0CD0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:40 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=692'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:40 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=692'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'What's the 5-day in Toronto, CA?' took 0.79s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Toronto', 'country_code': 'CA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fe899705-264a-4e9c-8499-93baf789f373', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day forecast Nairobi.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2DAEF0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2D80D0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:46 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=783'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:46 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=783'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day forecast Nairobi.' took 0.96s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Nairobi'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8f427b13-6f2d-47f7-a6d3-a1ed08851c71', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Week ahead in Auckland, NZ.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2FC790>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2FC0A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=805'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:52 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=805'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Week ahead in Auckland, NZ.' took 0.94s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Auckland', 'country_code': 'NZ'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3486ea69-fe05-413c-be11-b219272aef5f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for Reykjavik, IS (typo earlier).'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2DBF10>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2DBDC0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:11:58 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=548'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:11:58 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=548'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day for Reykjavik, IS (typo earlier).' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Reykjavik', 'country_code': 'IS'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cd248d97-937b-44bc-a3b8-54870f08ff14', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Forecast 5 days Lima, PE.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B11B0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2B2A70>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:12:04 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=991'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:12:04 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=991'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Forecast 5 days Lima, PE.' took 1.18s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Lima, PE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b8bec5bf-773f-44d3-9c32-e34340c22c72', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Next 5 days Johannesburg, ZA.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:12:10 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=821'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:12:10 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=821'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Next 5 days Johannesburg, ZA.' took 0.88s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Johannesburg', 'country_code': 'ZA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cd55abfe-ace1-4245-8c25-f85d5a78c2d0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Munich, DE 5-day outlook.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:12:16 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=505'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:12:16 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=505'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'Munich, DE 5-day outlook.' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Munich', 'country_code': 'DE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e6ba9bf0-669a-4d88-82c6-12ce5d946665', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'San Francisco, US-CA 5-day forecast.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2FF910>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A2FF190>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:12:21 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=579'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:12:21 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=579'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation 'San Francisco, US-CA 5-day forecast.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'San Francisco, US-CA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8d12b17a-dea5-4794-81b4-e6eb9a88692f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': '5-day for Porto, PT.'}], 'model': 'gemini-2.0-flash', 'temperature': 0.7, 'tool_choice': 'none', 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A3211B0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013F088D0E40> server_hostname='generativelanguage.googleapis.com' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000013F0A320A30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 22 Sep 2025 18:12:27 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=501'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions "200 OK" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Mon, 22 Sep 2025 18:12:27 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=501'), ('alt-svc', 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), ('transfer-encoding', 'chunked')])
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
DEBUG    test_performance:test_performance.py:232 5 day forecast conversation '5-day for Porto, PT.' took 0.63s with response: 
LLMResponse(type='function_call', content=None, function='get_forecast', module='meteorology', arguments={'city': 'Porto', 'country_code': 'PT'})


