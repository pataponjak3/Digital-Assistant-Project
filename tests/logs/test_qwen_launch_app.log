DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-077b925b-ef91-4fea-a690-0605d5ce5b45', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Steam.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44203C70>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442039A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:11 GMT'), (b'x-ratelimit-remaining-tokens', b'21947'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQiSH-3NKUce-983a50a98adad67c'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a50a98adad67c-IAD'), (b'etag', b'W/"2ac-qoIIYVR17nUs4L2xecEEHeOwM0E"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:11.823Z'), (b'x-api-call-start', b'2025-09-23T13:16:11.371Z'), (b'x-api-received', b'2025-09-23T13:16:11.351Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'JWkRneC5vMwK84CLJ05H0rbD0KunDJDSoZUMt1Esejj3CmM3UPuxGQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:11 GMT', 'x-ratelimit-remaining-tokens': '21947', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQiSH-3NKUce-983a50a98adad67c', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a50a98adad67c-IAD', 'etag': 'W/"2ac-qoIIYVR17nUs4L2xecEEHeOwM0E"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:11.823Z', 'x-api-call-start': '2025-09-23T13:16:11.371Z', 'x-api-received': '2025-09-23T13:16:11.351Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'JWkRneC5vMwK84CLJ05H0rbD0KunDJDSoZUMt1Esejj3CmM3UPuxGQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQiSH-3NKUce-983a50a98adad67c
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Steam.' took 2.00s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Steam', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-24684ee8-3e1b-458a-9e8c-35633dec80fc', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch dis cord.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44241BD0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44241660>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:17 GMT'), (b'x-ratelimit-remaining-tokens', b'27468'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQk7g-62bZhn-983a50cdbd2f82a2'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a50cdbd2f82a2-IAD'), (b'etag', b'W/"2ae-2lExYTmLwnKOtsZ8/pIckcQkvf4"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:17.407Z'), (b'x-api-call-start', b'2025-09-23T13:16:17.007Z'), (b'x-api-received', b'2025-09-23T13:16:16.999Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'bfaoBogGoebeAyeXr0UJkZkKhtIH-mV_QMu3n2Qmd6jvvuRQiycNVg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:17 GMT', 'x-ratelimit-remaining-tokens': '27468', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQk7g-62bZhn-983a50cdbd2f82a2', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a50cdbd2f82a2-IAD', 'etag': 'W/"2ae-2lExYTmLwnKOtsZ8/pIckcQkvf4"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:17.407Z', 'x-api-call-start': '2025-09-23T13:16:17.007Z', 'x-api-received': '2025-09-23T13:16:16.999Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'bfaoBogGoebeAyeXr0UJkZkKhtIH-mV_QMu3n2Qmd6jvvuRQiycNVg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQk7g-62bZhn-983a50cdbd2f82a2
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch dis cord.' took 0.73s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Discord', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0842731b-e4f9-4a4e-a14c-960df5ebc057', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Eclipse IDE.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44243A60>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44243640>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:23 GMT'), (b'x-ratelimit-remaining-tokens', b'32976'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQmoo-66dFFu-983a50f17b41e5e4'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a50f17b41e5e4-IAD'), (b'etag', b'W/"2b2-X168UEjIvL3sJA3PMuDJL3k/xrI"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:23.073Z'), (b'x-api-call-start', b'2025-09-23T13:16:22.702Z'), (b'x-api-received', b'2025-09-23T13:16:22.690Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'kIoJLc6rDA-618xIdG819-2KmCBDBs30S6lQhmm7im2Au70dwq19KQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:23 GMT', 'x-ratelimit-remaining-tokens': '32976', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQmoo-66dFFu-983a50f17b41e5e4', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a50f17b41e5e4-IAD', 'etag': 'W/"2b2-X168UEjIvL3sJA3PMuDJL3k/xrI"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:23.073Z', 'x-api-call-start': '2025-09-23T13:16:22.702Z', 'x-api-received': '2025-09-23T13:16:22.690Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'kIoJLc6rDA-618xIdG819-2KmCBDBs30S6lQhmm7im2Au70dwq19KQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQmoo-66dFFu-983a50f17b41e5e4
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start Eclipse IDE.' took 0.66s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Eclipse IDE', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-102cbf9b-ed09-410a-9d92-bf767d752514', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open VS Code.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44243100>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442431C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:28 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQoVj-66dFFu-983a51150de5d69c'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a51150de5d69c-IAD'), (b'etag', b'W/"2ae-mAw+3kwsR/RqZ4GS3UW+B1Q1JMw"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:28.877Z'), (b'x-api-call-start', b'2025-09-23T13:16:28.383Z'), (b'x-api-received', b'2025-09-23T13:16:28.370Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 191181f299c93f856cc1cdad79c1bb76.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'XOmeI7gzZdJ4umcC3-cGUyzsdNOY-a7YsMtKvdjtaIgokt6obaOKew==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:28 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQoVj-66dFFu-983a51150de5d69c', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a51150de5d69c-IAD', 'etag': 'W/"2ae-mAw+3kwsR/RqZ4GS3UW+B1Q1JMw"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:28.877Z', 'x-api-call-start': '2025-09-23T13:16:28.383Z', 'x-api-received': '2025-09-23T13:16:28.370Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 191181f299c93f856cc1cdad79c1bb76.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'XOmeI7gzZdJ4umcC3-cGUyzsdNOY-a7YsMtKvdjtaIgokt6obaOKew=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQoVj-66dFFu-983a51150de5d69c
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open VS Code.' took 0.78s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'VS Code', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7100bdfc-97b7-4629-8bf0-5cb91eedf143', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Run visual studio code pls.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442A1A80>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442A1660>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:34 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQqFD-3NKUce-983a513a6bc1778a'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a513a6bc1778a-IAD'), (b'etag', b'W/"2b9-kIdLvKRu0Egb7HWX21qSRcicHgo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:34.683Z'), (b'x-api-call-start', b'2025-09-23T13:16:34.274Z'), (b'x-api-received', b'2025-09-23T13:16:34.257Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'coFyWYUROJhSf6lxdvKs0SE9BeJdhcQzjsSaVryL5PRt27ybc_jXYA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:34 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQqFD-3NKUce-983a513a6bc1778a', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a513a6bc1778a-IAD', 'etag': 'W/"2b9-kIdLvKRu0Egb7HWX21qSRcicHgo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:34.683Z', 'x-api-call-start': '2025-09-23T13:16:34.274Z', 'x-api-received': '2025-09-23T13:16:34.257Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'coFyWYUROJhSf6lxdvKs0SE9BeJdhcQzjsSaVryL5PRt27ybc_jXYA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQqFD-3NKUce-983a513a6bc1778a
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Run visual studio code pls.' took 0.94s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Visual Studio Code', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-660b62eb-5a7f-4835-9510-be5cee1704a9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch VSC.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442A37F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442A33D0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:40 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQs1E-66dFFu-983a515ea9ee878e'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a515ea9ee878e-IAD'), (b'etag', b'W/"2ab-VDaiKd4T3K25YoDOfdO+xQasC4Q"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:40.686Z'), (b'x-api-call-start', b'2025-09-23T13:16:40.184Z'), (b'x-api-received', b'2025-09-23T13:16:40.174Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'8LRjLER0sg9o4GuEz2Jau8mXY8SqKr5r_QQ3HRN-mZ6hs_POpIOWag==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:40 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQs1E-66dFFu-983a515ea9ee878e', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a515ea9ee878e-IAD', 'etag': 'W/"2ab-VDaiKd4T3K25YoDOfdO+xQasC4Q"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:40.686Z', 'x-api-call-start': '2025-09-23T13:16:40.184Z', 'x-api-received': '2025-09-23T13:16:40.174Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '8LRjLER0sg9o4GuEz2Jau8mXY8SqKr5r_QQ3HRN-mZ6hs_POpIOWag=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQs1E-66dFFu-983a515ea9ee878e
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch VSC.' took 0.86s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'VSC', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a21248ac-2d6c-415c-8b27-ed86929e41e7', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Win RAR.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BD5D0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BD3F0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:46 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQtjw-3NKUce-983a51831bdc2431'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a51831bdc2431-IAD'), (b'etag', b'W/"2ad-KTcQMJ2ZU2uvgbaB0eOERH5A88w"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:46.473Z'), (b'x-api-call-start', b'2025-09-23T13:16:46.029Z'), (b'x-api-received', b'2025-09-23T13:16:46.014Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 923c2d2f00d1c4aa09564e5db1f3c7a8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'yZ46EZjpbCLwjZZ4t659SBJRD-xBMLl6ap5DGdd8bk7wZllYpBhWYQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:46 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQtjw-3NKUce-983a51831bdc2431', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a51831bdc2431-IAD', 'etag': 'W/"2ad-KTcQMJ2ZU2uvgbaB0eOERH5A88w"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:46.473Z', 'x-api-call-start': '2025-09-23T13:16:46.029Z', 'x-api-received': '2025-09-23T13:16:46.014Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 923c2d2f00d1c4aa09564e5db1f3c7a8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'yZ46EZjpbCLwjZZ4t659SBJRD-xBMLl6ap5DGdd8bk7wZllYpBhWYQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQtjw-3NKUce-983a51831bdc2431
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Win RAR.' took 0.89s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Win RAR', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d704d766-2c54-410d-aaac-4dbe60a4d483', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start WinRAR.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BF340>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BEF20>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:52 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQvVt-62bZhn-983a51a848c9c963'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a51a848c9c963-IAD'), (b'etag', b'W/"2ac-hvQJ+SRKYzIv2CpO9gpK/CZ/E7g"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:52.384Z'), (b'x-api-call-start', b'2025-09-23T13:16:51.935Z'), (b'x-api-received', b'2025-09-23T13:16:51.927Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'XlebuXQ0aUU3bUwi22uAlVtoddGysEtK-_nE5dtQq0JztLRfnnyDKg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:52 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQvVt-62bZhn-983a51a848c9c963', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a51a848c9c963-IAD', 'etag': 'W/"2ac-hvQJ+SRKYzIv2CpO9gpK/CZ/E7g"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:52.384Z', 'x-api-call-start': '2025-09-23T13:16:51.935Z', 'x-api-received': '2025-09-23T13:16:51.927Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'XlebuXQ0aUU3bUwi22uAlVtoddGysEtK-_nE5dtQq0JztLRfnnyDKg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQvVt-62bZhn-983a51a848c9c963
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start WinRAR.' took 0.80s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'WinRAR', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-50907537-14b0-4ceb-a283-08b63e11a294', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Google Chorme.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:16:58 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQxCm-3NKUce-983a51cbe9ea86d6'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a51cbe9ea86d6-IAD'), (b'etag', b'W/"2b3-k2dyWZF+XGJgcffHHrJUhIsz0NM"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:16:58.136Z'), (b'x-api-call-start', b'2025-09-23T13:16:57.685Z'), (b'x-api-received', b'2025-09-23T13:16:57.663Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Dt2fQxvlFIaziW0fgkoyAM_FkU6t0Xpwjb9uJ5L7d_uE9VcCxaXsSA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:16:58 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQxCm-3NKUce-983a51cbe9ea86d6', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a51cbe9ea86d6-IAD', 'etag': 'W/"2b3-k2dyWZF+XGJgcffHHrJUhIsz0NM"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:16:58.136Z', 'x-api-call-start': '2025-09-23T13:16:57.685Z', 'x-api-received': '2025-09-23T13:16:57.663Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Dt2fQxvlFIaziW0fgkoyAM_FkU6t0Xpwjb9uJ5L7d_uE9VcCxaXsSA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQxCm-3NKUce-983a51cbe9ea86d6
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Google Chorme.' took 0.87s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Google Chrome', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1f244fc8-6202-41ee-aa67-30f15380f7f6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch chrome browser.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BCD60>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442BCF10>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:17:03 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPQyxD-3NKUce-983a51f0ee400aba'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a51f0ee400aba-IAD'), (b'etag', b'W/"2ad-obTmP9KUv7Rdotef+EwSYSzgL2k"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:17:03.962Z'), (b'x-api-call-start', b'2025-09-23T13:17:03.558Z'), (b'x-api-received', b'2025-09-23T13:17:03.546Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'aO9b7ITMvOCbK-qyU_-Da5XYOAF30GlZc9oEBni2PWwsGdeLHUY94Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:17:03 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPQyxD-3NKUce-983a51f0ee400aba', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a51f0ee400aba-IAD', 'etag': 'W/"2ad-obTmP9KUv7Rdotef+EwSYSzgL2k"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:17:03.962Z', 'x-api-call-start': '2025-09-23T13:17:03.558Z', 'x-api-received': '2025-09-23T13:17:03.546Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'aO9b7ITMvOCbK-qyU_-Da5XYOAF30GlZc9oEBni2PWwsGdeLHUY94Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPQyxD-3NKUce-983a51f0ee400aba
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch chrome browser.' took 0.70s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'chrome', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-631c7373-d8ae-418c-b739-5fbcb937554c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Firefox.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442D51B0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B442D4D90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:17:09 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPR1hS-66dFFu-983a52152d791767'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a52152d791767-IAD'), (b'etag', b'W/"2ae-E2RjK10S57gWH8o38qkrqpP78HE"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:17:09.948Z'), (b'x-api-call-start', b'2025-09-23T13:17:09.429Z'), (b'x-api-received', b'2025-09-23T13:17:09.417Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'AE9eAkz1pUDg4OphdqMc7VUY9FMkzCWRc--TnwieZ8eDjjxCMTT5tw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:17:09 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPR1hS-66dFFu-983a52152d791767', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a52152d791767-IAD', 'etag': 'W/"2ae-E2RjK10S57gWH8o38qkrqpP78HE"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:17:09.948Z', 'x-api-call-start': '2025-09-23T13:17:09.429Z', 'x-api-received': '2025-09-23T13:17:09.417Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'AE9eAkz1pUDg4OphdqMc7VUY9FMkzCWRc--TnwieZ8eDjjxCMTT5tw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPR1hS-66dFFu-983a52152d791767
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Firefox.' took 1.07s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Firefox', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ee37881e-1caa-4afd-84fa-a6c1448adbce', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Notepad.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019691FD3CD0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019691FD3A00>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:21 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcktq-66dFFu-983a8a066dc39bd3'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8a066dc39bd3-IAD'), (b'etag', b'W/"2ae-UzVR4Vo+JGQF8SmneuCbk12CpBo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:21.436Z'), (b'x-api-call-start', b'2025-09-23T13:55:20.973Z'), (b'x-api-received', b'2025-09-23T13:55:20.962Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'tnXlUQHkZNi47sqolJwm7TdxYg7JsWdkJDrJ3RQX2JozIjp14RwXXQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:21 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcktq-66dFFu-983a8a066dc39bd3', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8a066dc39bd3-IAD', 'etag': 'W/"2ae-UzVR4Vo+JGQF8SmneuCbk12CpBo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:21.436Z', 'x-api-call-start': '2025-09-23T13:55:20.973Z', 'x-api-received': '2025-09-23T13:55:20.962Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'tnXlUQHkZNi47sqolJwm7TdxYg7JsWdkJDrJ3RQX2JozIjp14RwXXQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcktq-66dFFu-983a8a066dc39bd3
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start Notepad.' took 2.07s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Notepad', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8abeb44c-eb33-4c8e-9c36-3fbbdf036372', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch calc.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692015C30>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000196920156C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:27 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcnjf-62bZhn-983a8a2e4f6ed6f1'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8a2e4f6ed6f1-IAD'), (b'etag', b'W/"2ab-fueFiRaugdY1lKkkY2n8F85OGsY"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:27.587Z'), (b'x-api-call-start', b'2025-09-23T13:55:27.170Z'), (b'x-api-received', b'2025-09-23T13:55:27.158Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Uz-rngFIqVLX2NTB4Q4Dxr3uXpcpDxheqZzPUp6uTOXWRx1Qe5gNJQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:27 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcnjf-62bZhn-983a8a2e4f6ed6f1', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8a2e4f6ed6f1-IAD', 'etag': 'W/"2ab-fueFiRaugdY1lKkkY2n8F85OGsY"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:27.587Z', 'x-api-call-start': '2025-09-23T13:55:27.170Z', 'x-api-received': '2025-09-23T13:55:27.158Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Uz-rngFIqVLX2NTB4Q4Dxr3uXpcpDxheqZzPUp6uTOXWRx1Qe5gNJQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcnjf-62bZhn-983a8a2e4f6ed6f1
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch calc.' took 1.12s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'calc', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-40a648aa-0e33-422d-bfe4-2542faae8fe9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open VLC.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:33 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcpRq-3NKUce-983a8a523883c0d6'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8a523883c0d6-IAD'), (b'etag', b'W/"2a9-HjLEOn8uMISWm8Yylkued2EQBv0"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:33.347Z'), (b'x-api-call-start', b'2025-09-23T13:55:32.864Z'), (b'x-api-received', b'2025-09-23T13:55:32.852Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'mUuM8lEIoXkxfA_JMInozLJiFtqtekp8rMCd4GubMIiR5AJJz05fFg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:33 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcpRq-3NKUce-983a8a523883c0d6', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8a523883c0d6-IAD', 'etag': 'W/"2a9-HjLEOn8uMISWm8Yylkued2EQBv0"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:33.347Z', 'x-api-call-start': '2025-09-23T13:55:32.864Z', 'x-api-received': '2025-09-23T13:55:32.852Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'mUuM8lEIoXkxfA_JMInozLJiFtqtekp8rMCd4GubMIiR5AJJz05fFg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcpRq-3NKUce-983a8a523883c0d6
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open VLC.' took 0.83s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'VLC', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b1b99176-e496-409d-a479-194c91cee7e9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Spotify.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692075450>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692075030>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:39 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcrGW-66dFFu-983a8a77281bf286'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8a77281bf286-IAD'), (b'etag', b'W/"2ae-LdFegISBXsvpM7fEoTDQbRYaVvE"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:39.552Z'), (b'x-api-call-start', b'2025-09-23T13:55:39.054Z'), (b'x-api-received', b'2025-09-23T13:55:39.039Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'DNfrEeilgdApi81ReCHOb5wX0DAbIO1oyuHsu7zqYd77wwBUn3lxAg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:39 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcrGW-66dFFu-983a8a77281bf286', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8a77281bf286-IAD', 'etag': 'W/"2ae-LdFegISBXsvpM7fEoTDQbRYaVvE"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:39.552Z', 'x-api-call-start': '2025-09-23T13:55:39.054Z', 'x-api-received': '2025-09-23T13:55:39.039Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'DNfrEeilgdApi81ReCHOb5wX0DAbIO1oyuHsu7zqYd77wwBUn3lxAg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcrGW-66dFFu-983a8a77281bf286
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start Spotify.' took 1.13s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Spotify', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-254f1ee7-78b0-4d2c-85b7-076ab2d16557', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Adobe Reader.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:45 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPct2T-3NKUce-983a8a9d3d3590be'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8a9d3d3590be-IAD'), (b'etag', b'W/"2b2-qKtXR/NfmL9ML1TOs1vwQ2ZTmSY"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:45.382Z'), (b'x-api-call-start', b'2025-09-23T13:55:44.964Z'), (b'x-api-received', b'2025-09-23T13:55:44.952Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'CyOQrmgK6IgMYE7pS7ImB3IYf9dNQCXKJojeQmdtNP-K1scbYZ3UvA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:45 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPct2T-3NKUce-983a8a9d3d3590be', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8a9d3d3590be-IAD', 'etag': 'W/"2b2-qKtXR/NfmL9ML1TOs1vwQ2ZTmSY"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:45.382Z', 'x-api-call-start': '2025-09-23T13:55:44.964Z', 'x-api-received': '2025-09-23T13:55:44.952Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'CyOQrmgK6IgMYE7pS7ImB3IYf9dNQCXKJojeQmdtNP-K1scbYZ3UvA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPct2T-3NKUce-983a8a9d3d3590be
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Adobe Reader.' took 0.72s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Adobe Reader', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-94a199ed-cd29-4cd1-bfa7-7c0eb1e7d9db', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Acrobat.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692076B90>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692077BB0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:51 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcuie-66dFFu-983a8ac11f57e5f6'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8ac11f57e5f6-IAD'), (b'etag', b'W/"2ae-T1VkGDAy5+m6/XvFwegmHhEld/w"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:51.098Z'), (b'x-api-call-start', b'2025-09-23T13:55:50.660Z'), (b'x-api-received', b'2025-09-23T13:55:50.647Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 39ae765868f39f2168989dfa478b9354.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'-m7UgqmHvYdXRaqfBxRCNi6Ca8UmSKzpkxehFps4vULZnBv433NLuA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:51 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcuie-66dFFu-983a8ac11f57e5f6', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8ac11f57e5f6-IAD', 'etag': 'W/"2ae-T1VkGDAy5+m6/XvFwegmHhEld/w"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:51.098Z', 'x-api-call-start': '2025-09-23T13:55:50.660Z', 'x-api-received': '2025-09-23T13:55:50.647Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 39ae765868f39f2168989dfa478b9354.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '-m7UgqmHvYdXRaqfBxRCNi6Ca8UmSKzpkxehFps4vULZnBv433NLuA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcuie-66dFFu-983a8ac11f57e5f6
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch Acrobat.' took 0.83s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Acrobat', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f5400088-98d0-4f38-8557-c05d9ff49ebe', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Slack.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692017190>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000196920171C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:55:57 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcwUt-66dFFu-983a8ae5d99e05b9'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8ae5d99e05b9-IAD'), (b'etag', b'W/"2ab-egGByDV0qJAqCWyTtmqz6jgUgwg"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:55:57.116Z'), (b'x-api-call-start', b'2025-09-23T13:55:56.594Z'), (b'x-api-received', b'2025-09-23T13:55:56.577Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5a9df1bcd5f48109e94a8e34d807b686.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'VxqSQ3Gp4geA6P5JsfUZZ9zoJIWgcvLQ3jSuey5Z7V4fTX-qAeLeTg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:55:57 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcwUt-66dFFu-983a8ae5d99e05b9', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8ae5d99e05b9-IAD', 'etag': 'W/"2ab-egGByDV0qJAqCWyTtmqz6jgUgwg"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:55:57.116Z', 'x-api-call-start': '2025-09-23T13:55:56.594Z', 'x-api-received': '2025-09-23T13:55:56.577Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5a9df1bcd5f48109e94a8e34d807b686.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'VxqSQ3Gp4geA6P5JsfUZZ9zoJIWgcvLQ3jSuey5Z7V4fTX-qAeLeTg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcwUt-66dFFu-983a8ae5d99e05b9
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Slack.' took 1.03s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Slack', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-be8c2de6-a7da-460c-a705-4ecda14dbf12', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start MS Teams.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692085660>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019692085240>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:56:03 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPcyKa-66dFFu-983a8b0b8f8e1e2d'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8b0b8f8e1e2d-IAD'), (b'etag', b'W/"2ae-/Tnz1JZ94nOzkERRUgLmzQaFuhw"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:56:03.212Z'), (b'x-api-call-start', b'2025-09-23T13:56:02.777Z'), (b'x-api-received', b'2025-09-23T13:56:02.765Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'9ZZQLwoxP5RM5Dhvp8bQ1DqYPROJL9Q9Y73mgOYaN1ksr8PTVjVC1w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:56:03 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPcyKa-66dFFu-983a8b0b8f8e1e2d', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8b0b8f8e1e2d-IAD', 'etag': 'W/"2ae-/Tnz1JZ94nOzkERRUgLmzQaFuhw"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:56:03.212Z', 'x-api-call-start': '2025-09-23T13:56:02.777Z', 'x-api-received': '2025-09-23T13:56:02.765Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '9ZZQLwoxP5RM5Dhvp8bQ1DqYPROJL9Q9Y73mgOYaN1ksr8PTVjVC1w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPcyKa-66dFFu-983a8b0b8f8e1e2d
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start MS Teams.' took 1.04s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'MS Teams', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-01b67ed2-d4fa-440f-a926-8d8f026cc8d2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Outlok.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:56:08 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPd15M-3NKUce-983a8b30fbe6c596'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8b30fbe6c596-IAD'), (b'etag', b'W/"2ae-dkeQQOU3l66f5S+ONameBBWJ1zo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:56:09.077Z'), (b'x-api-call-start', b'2025-09-23T13:56:08.680Z'), (b'x-api-received', b'2025-09-23T13:56:08.669Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'AZQ89uOGrk1paPIZSJopAjDnscwHM5o_qPPNbR_O50oNQeqddaeB_w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:56:08 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPd15M-3NKUce-983a8b30fbe6c596', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8b30fbe6c596-IAD', 'etag': 'W/"2ae-dkeQQOU3l66f5S+ONameBBWJ1zo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:56:09.077Z', 'x-api-call-start': '2025-09-23T13:56:08.680Z', 'x-api-received': '2025-09-23T13:56:08.669Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'AZQ89uOGrk1paPIZSJopAjDnscwHM5o_qPPNbR_O50oNQeqddaeB_w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPd15M-3NKUce-983a8b30fbe6c596
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Outlok.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Outlook', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1ab457ad-8b44-40f5-a615-9aa066a34ee6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Word.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019693070D60>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000019691613D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019693070940>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:56:14 GMT'), (b'x-ratelimit-remaining-tokens', b'33194'), (b'x-ratelimit-reset', b'1'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPd2mx-3NKUce-983a8b552ad3c979'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a8b552ad3c979-IAD'), (b'etag', b'W/"2ab-qdcyOPscIkbznQrlal3vcneql3Y"'), (b'retry-after', b'1'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:56:14.834Z'), (b'x-api-call-start', b'2025-09-23T13:56:14.403Z'), (b'x-api-received', b'2025-09-23T13:56:14.388Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'198'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'WteeyDF2FuwYh5tk6etMWUYP1aWWH5WLeYGGEhEwhDkQ4hEO17lifA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:56:14 GMT', 'x-ratelimit-remaining-tokens': '33194', 'x-ratelimit-reset': '1', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPd2mx-3NKUce-983a8b552ad3c979', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a8b552ad3c979-IAD', 'etag': 'W/"2ab-qdcyOPscIkbznQrlal3vcneql3Y"', 'retry-after': '1', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:56:14.834Z', 'x-api-call-start': '2025-09-23T13:56:14.403Z', 'x-api-received': '2025-09-23T13:56:14.388Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '198', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'WteeyDF2FuwYh5tk6etMWUYP1aWWH5WLeYGGEhEwhDkQ4hEO17lifA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPd2mx-3NKUce-983a8b552ad3c979
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch Word.' took 0.79s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Word', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4d775858-def5-4c6a-b320-06c285f84bd6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Excel.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44390F40>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44390B20>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:08 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRJxp-62bZhn-983a53807efab560'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a53807efab560-IAD'), (b'etag', b'W/"2ac-oqojJSW8+eNRGGiaYyX6pWCLMGo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:07.991Z'), (b'x-api-call-start', b'2025-09-23T13:18:07.514Z'), (b'x-api-received', b'2025-09-23T13:18:07.498Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'1Zu9frxC0cHTYOwTinEFV0l3hEo1RKLktmbiO2S-atebLDMei44sjw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:08 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRJxp-62bZhn-983a53807efab560', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a53807efab560-IAD', 'etag': 'W/"2ac-oqojJSW8+eNRGGiaYyX6pWCLMGo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:07.991Z', 'x-api-call-start': '2025-09-23T13:18:07.514Z', 'x-api-received': '2025-09-23T13:18:07.498Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '1Zu9frxC0cHTYOwTinEFV0l3hEo1RKLktmbiO2S-atebLDMei44sjw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRJxp-62bZhn-983a53807efab560
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Excel.' took 0.93s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Excel', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cff1889f-48c5-4812-bf41-eb822e60ef1c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Power Point.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44392CB0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44392890>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:13 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRLkR-66dFFu-983a53a58c5c2996'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a53a58c5c2996-IAD'), (b'etag', b'W/"2b1-oY84suZULaiPA9LF84Y+7g683fI"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:13.950Z'), (b'x-api-call-start', b'2025-09-23T13:18:13.516Z'), (b'x-api-received', b'2025-09-23T13:18:13.506Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'iOK6J3Wzn8MnCJaN6bbquuckomRohVav7tyLugoziAitHxJRDbbpzg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:13 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRLkR-66dFFu-983a53a58c5c2996', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a53a58c5c2996-IAD', 'etag': 'W/"2b1-oY84suZULaiPA9LF84Y+7g683fI"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:13.950Z', 'x-api-call-start': '2025-09-23T13:18:13.516Z', 'x-api-received': '2025-09-23T13:18:13.506Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'iOK6J3Wzn8MnCJaN6bbquuckomRohVav7tyLugoziAitHxJRDbbpzg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRLkR-66dFFu-983a53a58c5c2996
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start Power Point.' took 0.92s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Power Point', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8f64f7f4-1a8b-47bc-b38e-b0feca2b98c1', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Paint.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44390B50>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B44390A90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:19 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRNWV-66dFFu-983a53cb19d0c956'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a53cb19d0c956-IAD'), (b'etag', b'W/"2ac-lfpCiCVzhfxXsm+A7Xp0VT7Ht0g"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:19.908Z'), (b'x-api-call-start', b'2025-09-23T13:18:19.436Z'), (b'x-api-received', b'2025-09-23T13:18:19.426Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'bZyIco4YyRKKuZersrcT76gHHZeSamWXPT8AjeV2kxamSPCl80Vpeg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:19 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRNWV-66dFFu-983a53cb19d0c956', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a53cb19d0c956-IAD', 'etag': 'W/"2ac-lfpCiCVzhfxXsm+A7Xp0VT7Ht0g"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:19.908Z', 'x-api-call-start': '2025-09-23T13:18:19.436Z', 'x-api-received': '2025-09-23T13:18:19.426Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'bZyIco4YyRKKuZersrcT76gHHZeSamWXPT8AjeV2kxamSPCl80Vpeg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRNWV-66dFFu-983a53cb19d0c956
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Paint.' took 0.95s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Paint', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c015642b-e3a3-4c9c-97e9-488b6eb9388a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Edge browser.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443ADA50>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443AD630>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:25 GMT'), (b'x-ratelimit-remaining-tokens', b'29312'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRQGa-3NKUce-983a53efdf99ef62'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a53efdf99ef62-IAD'), (b'etag', b'W/"2ab-KJ7BEm33+ttSFYE99rZMoiGpljA"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:25.783Z'), (b'x-api-call-start', b'2025-09-23T13:18:25.363Z'), (b'x-api-received', b'2025-09-23T13:18:25.348Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'jWaUV7yQPu6ggieqo-sA6Bniz-oKNvhdvWhL1xCFKq-xcw3OCn22dA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:25 GMT', 'x-ratelimit-remaining-tokens': '29312', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRQGa-3NKUce-983a53efdf99ef62', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a53efdf99ef62-IAD', 'etag': 'W/"2ab-KJ7BEm33+ttSFYE99rZMoiGpljA"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:25.783Z', 'x-api-call-start': '2025-09-23T13:18:25.363Z', 'x-api-received': '2025-09-23T13:18:25.348Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4ccea9891122bbc59cea4168a401fd44.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'jWaUV7yQPu6ggieqo-sA6Bniz-oKNvhdvWhL1xCFKq-xcw3OCn22dA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRQGa-3NKUce-983a53efdf99ef62
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start Edge browser.' took 0.77s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Edge', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-015a9c06-f143-43e1-8795-db7e4240fefd', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Git Extensions.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443AF7C0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443AF3A0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:31 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRRyd-62bZhn-983a54140c618b8b'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a54140c618b8b-IAD'), (b'etag', b'W/"2b6-xNHDJeEVVH1k8cYuXbSyhZrkS2c"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:31.542Z'), (b'x-api-call-start', b'2025-09-23T13:18:31.100Z'), (b'x-api-received', b'2025-09-23T13:18:31.093Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'qgVnsY0aKudMQ20pCjtyQB0DkOuk_p7Hol8Kw-kWL02EW8ckOtEOAg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:31 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRRyd-62bZhn-983a54140c618b8b', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a54140c618b8b-IAD', 'etag': 'W/"2b6-xNHDJeEVVH1k8cYuXbSyhZrkS2c"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:31.542Z', 'x-api-call-start': '2025-09-23T13:18:31.100Z', 'x-api-received': '2025-09-23T13:18:31.093Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'qgVnsY0aKudMQ20pCjtyQB0DkOuk_p7Hol8Kw-kWL02EW8ckOtEOAg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRRyd-62bZhn-983a54140c618b8b
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch Git Extensions.' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Git Extensions', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e884586d-da4d-4d55-81eb-be85039316fc', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open 7Zip.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443C1570>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443C1150>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:37 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRTg8-3NKUce-983a54380d09f282'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a54380d09f282-IAD'), (b'etag', b'W/"2ac-WKramqHaeBKEOJiEEXE3aQUn77Q"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:37.236Z'), (b'x-api-call-start', b'2025-09-23T13:18:36.819Z'), (b'x-api-received', b'2025-09-23T13:18:36.806Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42bf01bb5b494f9d7ad3dd5810b5a212.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'TbH2Ai6fw4--2dwEjdO06PQnR0K3xT4RYD_l-5fiG9sivy2sEUHiAw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:37 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRTg8-3NKUce-983a54380d09f282', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a54380d09f282-IAD', 'etag': 'W/"2ac-WKramqHaeBKEOJiEEXE3aQUn77Q"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:37.236Z', 'x-api-call-start': '2025-09-23T13:18:36.819Z', 'x-api-received': '2025-09-23T13:18:36.806Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42bf01bb5b494f9d7ad3dd5810b5a212.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'TbH2Ai6fw4--2dwEjdO06PQnR0K3xT4RYD_l-5fiG9sivy2sEUHiAw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRTg8-3NKUce-983a54380d09f282
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open 7Zip.' took 0.83s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': '7Zip', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0757f7ba-aa2f-423a-aa78-d20326e76437', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start IntelliJ.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443AD270>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443ADBA0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:43 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRVQM-3NKUce-983a545c6acd2265'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a545c6acd2265-IAD'), (b'etag', b'W/"2ae-TsPD2B6jtGhLZFTZNaPdtbFxD1Q"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:43.065Z'), (b'x-api-call-start', b'2025-09-23T13:18:42.631Z'), (b'x-api-received', b'2025-09-23T13:18:42.618Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'bZkNI6cq3SYSzaXGS3gbJKvvnkYPeLdXIE0b9pSw8A92DtKF2VTSKg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:43 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRVQM-3NKUce-983a545c6acd2265', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a545c6acd2265-IAD', 'etag': 'W/"2ae-TsPD2B6jtGhLZFTZNaPdtbFxD1Q"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:43.065Z', 'x-api-call-start': '2025-09-23T13:18:42.631Z', 'x-api-received': '2025-09-23T13:18:42.618Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 347b4531a9eb19c96c462a85600ac33a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'bZkNI6cq3SYSzaXGS3gbJKvvnkYPeLdXIE0b9pSw8A92DtKF2VTSKg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRVQM-3NKUce-983a545c6acd2265
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Start IntelliJ.' took 0.83s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'IntelliJ', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d175e8ee-4e5f-4ac2-8c25-aabe5e4f3aac', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Pychram.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443C27D0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443C1870>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:49 GMT'), (b'x-ratelimit-remaining-tokens', b'33333'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRX98-3NKUce-983a5480f824ae2f'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a5480f824ae2f-IAD'), (b'etag', b'W/"2ae-PLQyCMJKifit/3Xs9GpmGBG4cKc"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:48.964Z'), (b'x-api-call-start', b'2025-09-23T13:18:48.474Z'), (b'x-api-received', b'2025-09-23T13:18:48.464Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'dPgolTTAKUQLBL_OtAJxXMPnIQh5gd3QTYJykvGXvBzHUm9s2ERz_A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:49 GMT', 'x-ratelimit-remaining-tokens': '33333', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRX98-3NKUce-983a5480f824ae2f', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a5480f824ae2f-IAD', 'etag': 'W/"2ae-PLQyCMJKifit/3Xs9GpmGBG4cKc"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:48.964Z', 'x-api-call-start': '2025-09-23T13:18:48.474Z', 'x-api-received': '2025-09-23T13:18:48.464Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2b1fd1e1421ca124eaa002817c6c475a.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'dPgolTTAKUQLBL_OtAJxXMPnIQh5gd3QTYJykvGXvBzHUm9s2ERz_A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRX98-3NKUce-983a5480f824ae2f
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Open Pychram.' took 0.80s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'PyChram', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3ad2e627-d091-4daf-97f8-db34b21a9ee7', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standart, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Android Studio.'}], 'model': 'Qwen/Qwen2.5-7B-Instruct', 'temperature': 0.7, 'tool_choice': None, 'tools': None, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443E0580>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000014B428B3D40> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000014B443E0160>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Tue, 23 Sep 2025 13:18:54 GMT'), (b'x-ratelimit-remaining-tokens', b'28702'), (b'x-ratelimit-reset', b'2'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'oCPRYtT-62bZhn-983a54a54b7bda52'), (b'x-inference-provider', b'together'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'983a54a54b7bda52-IAD'), (b'etag', b'W/"2b6-LXvCUdRlvPJmNftb1JCMBnENHUo"'), (b'retry-after', b'2'), (b'server', b'cloudflare'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains'), (b'x-api-call-end', b'2025-09-23T13:18:54.785Z'), (b'x-api-call-start', b'2025-09-23T13:18:54.352Z'), (b'x-api-received', b'2025-09-23T13:18:54.340Z'), (b'x-inference-version', b'v2'), (b'x-ratelimit', b'false'), (b'x-ratelimit-limit', b'100'), (b'x-ratelimit-limit-tokens', b'33333'), (b'x-ratelimit-remaining', b'199'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'cPoPdFtNWHtL9K-a7gcbD4Y72n5IWAj4paZWPEmg2dHL8-g95q-Plw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Tue, 23 Sep 2025 13:18:54 GMT', 'x-ratelimit-remaining-tokens': '28702', 'x-ratelimit-reset': '2', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'oCPRYtT-62bZhn-983a54a54b7bda52', 'x-inference-provider': 'together', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '983a54a54b7bda52-IAD', 'etag': 'W/"2b6-LXvCUdRlvPJmNftb1JCMBnENHUo"', 'retry-after': '2', 'server': 'cloudflare', 'strict-transport-security': 'max-age=15552000; includeSubDomains', 'x-api-call-end': '2025-09-23T13:18:54.785Z', 'x-api-call-start': '2025-09-23T13:18:54.352Z', 'x-api-received': '2025-09-23T13:18:54.340Z', 'x-inference-version': 'v2', 'x-ratelimit': 'false', 'x-ratelimit-limit': '100', 'x-ratelimit-limit-tokens': '33333', 'x-ratelimit-remaining': '199', 'x-cache': 'Miss from cloudfront', 'via': '1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'cPoPdFtNWHtL9K-a7gcbD4Y72n5IWAj4paZWPEmg2dHL8-g95q-Plw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: oCPRYtT-62bZhn-983a54a54b7bda52
DEBUG    test_performance:test_performance.py:336 Launch application conversation 'Launch Android Studio.' took 0.75s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Android Studio', 'is_sure_after_multiple_matches': False})


