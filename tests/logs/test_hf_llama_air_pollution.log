DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3e71e6a2-1fe7-4e42-a3d6-38fa976fc11a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Current air quality at 40.7128, -74.0060?'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125C5E0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125C310>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:57:46 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'f11427ee-96dc-4ce8-a434-d3f9ca0680c8'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b08501c010583-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.279'), (b'fireworks-server-time-to-first-token', b'0.068'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=417.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'280'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'OJjmVxy_OQ9mCC5Ok_kaVMjBsKg5DwTGSrb8UKTZLQA5cdHP43Ua6w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:57:46 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'f11427ee-96dc-4ce8-a434-d3f9ca0680c8', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b08501c010583-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.279', 'fireworks-server-time-to-first-token': '0.068', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=417.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '280', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 4c13f73e83aaf9d7bee2c3b379c641d6.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'OJjmVxy_OQ9mCC5Ok_kaVMjBsKg5DwTGSrb8UKTZLQA5cdHP43Ua6w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: f11427ee-96dc-4ce8-a434-d3f9ca0680c8
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Current air quality at 40.7128, -74.0060?' took 1.17s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 40.7128, 'lon': -74.006})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c29099e5-d6a9-4522-915c-ca7be19fbadf', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI near -33.8688, 151.2093 right now.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125E350>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125E0E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:57:51 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59476'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'0282a3b8-8c0f-4ea8-b3f8-eb6c7f4e1504'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0873ca55ea88-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'524'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.236'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=336.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'237'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'KnYNkPvAT2VZ_AQRm-MDvzU51O3ISMex_DI-Rk-zZjUzfmgxlReL7w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:57:51 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59476', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '0282a3b8-8c0f-4ea8-b3f8-eb6c7f4e1504', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0873ca55ea88-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '524', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.236', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=336.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '237', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'KnYNkPvAT2VZ_AQRm-MDvzU51O3ISMex_DI-Rk-zZjUzfmgxlReL7w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 0282a3b8-8c0f-4ea8-b3f8-eb6c7f4e1504
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI near -33.8688, 151.2093 right now.' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': -33.8688, 'lon': 151.2093})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8b6277aa-6f3a-40cd-8dda-36edafe1cd6c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': "What's PM2.5 at -23.5505, -46.6333?"}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:57:57 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59474'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'a38ae529-3dbd-47a4-8d7e-d2e4b2108be7'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b08966cbc8b3e-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'526'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.338'), (b'fireworks-server-time-to-first-token', b'0.054'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=521.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'339'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'KL1L_b9cFq9nIpNhceyhpZaAsy5m8aThe-JfmSNTPz2tNHDwH-BAZg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:57:57 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59474', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'a38ae529-3dbd-47a4-8d7e-d2e4b2108be7', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b08966cbc8b3e-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '526', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.338', 'fireworks-server-time-to-first-token': '0.054', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=521.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '339', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'KL1L_b9cFq9nIpNhceyhpZaAsy5m8aThe-JfmSNTPz2tNHDwH-BAZg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: a38ae529-3dbd-47a4-8d7e-d2e4b2108be7
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'What's PM2.5 at -23.5505, -46.6333?' took 0.72s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': -23.5505, 'lon': -46.6333})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fd32281d-18e1-4f95-bc0e-dc9c5da226cd', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air pollution for 30.0444, 31.2357.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A5B10>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A56F0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:03 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59478'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'1fa8824b-06cc-4226-a48b-517fc5205d6c'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b08bad8e6d639-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'522'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.176'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=291.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'178'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'yRktrIy9KRyR4oGU8Iygdbz68DMp3yvq0yF9fezpo58wbOBlXXoa-w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:03 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59478', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '1fa8824b-06cc-4226-a48b-517fc5205d6c', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b08bad8e6d639-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '522', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.176', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=291.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '178', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'yRktrIy9KRyR4oGU8Iygdbz68DMp3yvq0yF9fezpo58wbOBlXXoa-w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 1fa8824b-06cc-4226-a48b-517fc5205d6c
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air pollution for 30.0444, 31.2357.' took 0.59s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 30.0444, 'lon': 31.2357})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ac78d4ac-cc96-4223-864b-dda021ddb7b6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI around 55.7558, 37.6173 (Moscow).'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A7820>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A7400>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:08 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59475'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'f42325b1-a669-4e08-a43f-7200dd77c1bb'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b08dd8904170d-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'525'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.348'), (b'fireworks-server-time-to-first-token', b'0.059'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=473.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'349'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 8b0fd9a74745a3290a9c7ffef7d75076.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'cW7ltMGJKH2IetsxRPj2p0uvLvgCMOlyTtX5qBq7ysIHz_Uo7TlYRg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:08 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59475', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'f42325b1-a669-4e08-a43f-7200dd77c1bb', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b08dd8904170d-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '525', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.348', 'fireworks-server-time-to-first-token': '0.059', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=473.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '349', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 8b0fd9a74745a3290a9c7ffef7d75076.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'cW7ltMGJKH2IetsxRPj2p0uvLvgCMOlyTtX5qBq7ysIHz_Uo7TlYRg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: f42325b1-a669-4e08-a43f-7200dd77c1bb
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI around 55.7558, 37.6173 (Moscow).' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 55.7558, 'lon': 37.6173})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-149bd3a9-4ed6-41c6-9c9e-f215c7615f82', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Current AQI for -1.2921, 36.8219.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A5870>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712A5180>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:14 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'7ac9d503-f012-4f04-bef6-694b965e721e'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b090128989698-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.370'), (b'fireworks-server-time-to-first-token', b'0.055'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=481.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'372'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'FuxQ8K2QoeoryMCwR-VRn1FQEcW6eSHdwP_y4wlknnDXW8GAcY0IFw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:14 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '7ac9d503-f012-4f04-bef6-694b965e721e', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b090128989698-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.370', 'fireworks-server-time-to-first-token': '0.055', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=481.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '372', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'FuxQ8K2QoeoryMCwR-VRn1FQEcW6eSHdwP_y4wlknnDXW8GAcY0IFw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 7ac9d503-f012-4f04-bef6-694b965e721e
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Current AQI for -1.2921, 36.8219.' took 0.69s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': -1.2921, 'lon': 36.8219})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1f12b067-9d14-4ff2-8fcb-856c64f22fa0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Is air clean near 64.1466, -21.9426?'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712D4D30>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712D4910>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:20 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'ffd1e020-f1dd-4125-bb08-d110498d02f1'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0924cfc1f784-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.174'), (b'fireworks-server-time-to-first-token', b'0.039'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=380.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'177'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'5tfhn-q4-6S4gBPa6lAVaq5hkewEDn3JfjRmOChjKcFbeahFTSP6-w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:20 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'ffd1e020-f1dd-4125-bb08-d110498d02f1', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0924cfc1f784-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.174', 'fireworks-server-time-to-first-token': '0.039', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=380.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '177', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '5tfhn-q4-6S4gBPa6lAVaq5hkewEDn3JfjRmOChjKcFbeahFTSP6-w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: ffd1e020-f1dd-4125-bb08-d110498d02f1
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Is air clean near 64.1466, -21.9426?' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 64.1466, 'lon': -21.9426})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8652965a-2b14-49c8-b79f-827fef130a05', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Pollution stats at -33.9249, 18.4241.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:25 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59477'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'40d51f77-f19d-4d1a-8b02-5e0e15a2aa09'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b09479d79e645-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'523'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.230'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=342.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'231'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'lH29jsg3uWNAjG0bNnh7uzHtCT8-cXsToXzvfFubgmS5FwtOulv0ig==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:25 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59477', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '40d51f77-f19d-4d1a-8b02-5e0e15a2aa09', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b09479d79e645-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '523', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.230', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=342.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '231', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 e78b88048cb2f0beb893089a9fa30352.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'lH29jsg3uWNAjG0bNnh7uzHtCT8-cXsToXzvfFubgmS5FwtOulv0ig=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 40d51f77-f19d-4d1a-8b02-5e0e15a2aa09
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Pollution stats at -33.9249, 18.4241.' took 0.53s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': -33.9249, 'lon': 18.4241})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0552d35b-1e7d-478f-b3b1-d37ada007dad', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI now 35.6895, 139.6917.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712F0370>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712D7F10>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:31 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59478'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c932c1f3-ccbc-44b0-9461-061989907f17'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b096a9b0da471-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'522'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.400'), (b'fireworks-server-time-to-first-token', b'0.052'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=513.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'402'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'UhvpGtyEyks5boZDn5l_-UxpWPRgaMc_MIgsurWPqpHKJyRCbQzGwA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:31 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59478', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c932c1f3-ccbc-44b0-9461-061989907f17', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b096a9b0da471-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '522', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.400', 'fireworks-server-time-to-first-token': '0.052', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=513.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '402', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'UhvpGtyEyks5boZDn5l_-UxpWPRgaMc_MIgsurWPqpHKJyRCbQzGwA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c932c1f3-ccbc-44b0-9461-061989907f17
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI now 35.6895, 139.6917.' took 0.75s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 35.6895, 'lon': 139.6917})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b23de889-ce1d-4282-9912-05e836936d10', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality 38.7223, -9.1393.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:37 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59479'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'67128e5d-609c-49df-9247-cdae42fb19ee'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b098e2915388c-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'521'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.383'), (b'fireworks-server-time-to-first-token', b'0.055'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=515.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'385'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'h8dihdbMOAqngNRTzE_z8ivSVtCUNBVGzAJvMVA0W7B7Zf3NCw9ynA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:37 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59479', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '67128e5d-609c-49df-9247-cdae42fb19ee', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b098e2915388c-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '521', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.383', 'fireworks-server-time-to-first-token': '0.055', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=515.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '385', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'h8dihdbMOAqngNRTzE_z8ivSVtCUNBVGzAJvMVA0W7B7Zf3NCw9ynA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 67128e5d-609c-49df-9247-cdae42fb19ee
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality 38.7223, -9.1393.' took 0.69s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'lat': 38.7223, 'lon': -9.1393})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5cb37694-61b9-4d2b-8cc2-c82a503b813d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality for 10001, US.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125DF00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712D4A30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:43 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'603ec922-dc9e-4977-a171-fa509de3d35e'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b09b21fd6d629-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.488'), (b'fireworks-server-time-to-first-token', b'0.053'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=747.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'490'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 ebe6a6f6a4dd1a63623021825f18f4f8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'pmXdxxS0-yv78IE2-DJ08oTExKGNGjT3r9JnSnyIXjMf5Ny8QHFkmg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:43 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '603ec922-dc9e-4977-a171-fa509de3d35e', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b09b21fd6d629-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.488', 'fireworks-server-time-to-first-token': '0.053', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=747.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '490', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 ebe6a6f6a4dd1a63623021825f18f4f8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'pmXdxxS0-yv78IE2-DJ08oTExKGNGjT3r9JnSnyIXjMf5Ny8QHFkmg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 603ec922-dc9e-4977-a171-fa509de3d35e
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality for 10001, US.' took 0.97s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': '', 'lat': '', 'lon': '', 'zip': '10001', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4bfdd78c-afb3-46d5-a61f-a2c7880c552d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI now at 90210, US.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:48 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59483'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'74db2c42-6dac-467f-98dc-dc37fed83864'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b09d7b830d6df-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'517'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.161'), (b'fireworks-server-time-to-first-token', b'0.037'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=339.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'163'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 ebe6a6f6a4dd1a63623021825f18f4f8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'EpV-m52PDO9yDerHTVpVC6OGRdBqarl81BuopIJLg0lxkSN3mAYxbg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:48 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59483', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '74db2c42-6dac-467f-98dc-dc37fed83864', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b09d7b830d6df-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '517', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.161', 'fireworks-server-time-to-first-token': '0.037', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=339.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '163', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 ebe6a6f6a4dd1a63623021825f18f4f8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'EpV-m52PDO9yDerHTVpVC6OGRdBqarl81BuopIJLg0lxkSN3mAYxbg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 74db2c42-6dac-467f-98dc-dc37fed83864
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI now at 90210, US.' took 0.63s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '90210', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e579e0f2-e116-4754-8b1e-d80506c5ac36', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Pollution level 60614, US.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712F3FA0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712F3B80>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:58:54 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'b7ee3848-822f-49cb-be9e-4d055b462e4f'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b09fafdff2265-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.382'), (b'fireworks-server-time-to-first-token', b'0.052'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=485.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'384'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'm_iuMtOh6v_JPYonnExiOsY1WX3oOR1TbzuRwM_ZDQkL7ylcG39zgQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:58:54 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'b7ee3848-822f-49cb-be9e-4d055b462e4f', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b09fafdff2265-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.382', 'fireworks-server-time-to-first-token': '0.052', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=485.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '384', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'm_iuMtOh6v_JPYonnExiOsY1WX3oOR1TbzuRwM_ZDQkL7ylcG39zgQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: b7ee3848-822f-49cb-be9e-4d055b462e4f
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Pollution level 60614, US.' took 0.74s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '60614', 'country_code': 'US'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9df0f80d-fdbd-40cf-b64f-2c2e5029608b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI 10115, DE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:00 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'1d3468cf-7688-4752-a0bc-11e4e7bcd413'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0a1e681ed63c-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.360'), (b'fireworks-server-time-to-first-token', b'0.079'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=474.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'361'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'DApl-Awj3Uv47TngJ6LFW2loIy_-9iDCymUDk9ok_dlComLMwe2odw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:00 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '1d3468cf-7688-4752-a0bc-11e4e7bcd413', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0a1e681ed63c-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.360', 'fireworks-server-time-to-first-token': '0.079', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=474.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '361', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'DApl-Awj3Uv47TngJ6LFW2loIy_-9iDCymUDk9ok_dlComLMwe2odw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 1d3468cf-7688-4752-a0bc-11e4e7bcd413
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI 10115, DE.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '10115', 'country_code': 'DE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cb4cbda3-167d-4032-993d-f7b317c3ec5b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality SW1A 1AA, GB.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7130B5E0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7130B1C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:05 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59482'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'30d7dd54-e65a-44c7-ab89-60904e4d470d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0a41fb47d739-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'518'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.209'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=365.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'211'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 229645111d244444bdf50dba19c1f2c8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'7rTXfiQUUYtwASMMriuSKUUnDn7f1Wtqe_ekamFdZMJjvZ-jfF6r0g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:05 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59482', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '30d7dd54-e65a-44c7-ab89-60904e4d470d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0a41fb47d739-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '518', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.209', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=365.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '211', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 229645111d244444bdf50dba19c1f2c8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '7rTXfiQUUYtwASMMriuSKUUnDn7f1Wtqe_ekamFdZMJjvZ-jfF6r0g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 30d7dd54-e65a-44c7-ab89-60904e4d470d
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality SW1A 1AA, GB.' took 0.58s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': 'SW1A 1AA', 'country_code': 'GB'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9279657f-f4f9-44d3-8143-543880bcb221', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI 75001, FR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712F16F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C712F2410>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:11 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'973bd9ff-e988-4e23-bb3c-bd6f976b565b'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0a64f90881bb-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.243'), (b'fireworks-server-time-to-first-token', b'0.078'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=660.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'245'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'vHj1z9lqFMoHYxD7sotqdAiugMjtbanbI-_PS1acs4lSpRUF0KDT2w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:11 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '973bd9ff-e988-4e23-bb3c-bd6f976b565b', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0a64f90881bb-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.243', 'fireworks-server-time-to-first-token': '0.078', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=660.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '245', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'vHj1z9lqFMoHYxD7sotqdAiugMjtbanbI-_PS1acs4lSpRUF0KDT2w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 973bd9ff-e988-4e23-bb3c-bd6f976b565b
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI 75001, FR.' took 0.90s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '75001', 'country_code': 'FR'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5dafc215-4b5a-4c27-8f9a-f68c1cc35075', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality 1250-096, PT.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71309840>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7130A7D0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:17 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59483'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'a86624d9-a48f-4238-ae5b-dabd97ed8697'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0a89ea2e170d-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'517'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.281'), (b'fireworks-server-time-to-first-token', b'0.051'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=374.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'282'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'NDiZw_xxKaJWq-a1NdfgqPNeFnVDE_CVzZEH-sY7nUR1ZY7s74td8g==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:17 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59483', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'a86624d9-a48f-4238-ae5b-dabd97ed8697', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0a89ea2e170d-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '517', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.281', 'fireworks-server-time-to-first-token': '0.051', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=374.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '282', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 07b26b5e851ab857ec87e6df0aa7882e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'NDiZw_xxKaJWq-a1NdfgqPNeFnVDE_CVzZEH-sY7nUR1ZY7s74td8g=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: a86624d9-a48f-4238-ae5b-dabd97ed8697
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality 1250-096, PT.' took 0.59s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '1250-096', 'country_code': 'PT'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d023a5fc-798f-4eca-bfdc-1f096a3c85ca', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI 2000, AU.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71339000>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71338BE0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:22 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'e0457db9-428e-41bb-83db-f69d71c07d40'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0aacde8cd962-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.244'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=393.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'248'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'0ZY5MiFYayzGImdacXLNzX1-qOZ85uC1ueTodWWh3xA3XpMQD4cNKA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:22 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'e0457db9-428e-41bb-83db-f69d71c07d40', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0aacde8cd962-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.244', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=393.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '248', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '0ZY5MiFYayzGImdacXLNzX1-qOZ85uC1ueTodWWh3xA3XpMQD4cNKA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: e0457db9-428e-41bb-83db-f69d71c07d40
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI 2000, AU.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Canberra', 'country_code': 'AU'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d3a62797-26b6-4c75-b05e-c77a549fd0d2', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality 01000-000, BR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:28 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59483'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'945573a8-d961-4303-ae36-e556f8bc0079'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0ad0386a99ad-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'517'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.284'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=408.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'286'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'mSon-Cokjp6Ax2-aqv-YXbvhQSdG49JBcqtWfR6naxZU7Z1SKRpwoQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:28 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59483', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '945573a8-d961-4303-ae36-e556f8bc0079', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0ad0386a99ad-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '517', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.284', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=408.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '286', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'mSon-Cokjp6Ax2-aqv-YXbvhQSdG49JBcqtWfR6naxZU7Z1SKRpwoQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 945573a8-d961-4303-ae36-e556f8bc0079
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality 01000-000, BR.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '01000-000', 'country_code': 'BR'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3b7dc289-3254-4a01-a44a-9ce088df9dcb', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI 110001, IN.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:34 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59485'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'a88830af-beba-481c-9ef9-f98935230590'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0af34ad4c93f-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'515'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.252'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=382.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'254'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'-b-u5FT8QnpLhp5dyuRfSO8WbSFLmUhFjv4cFnY9mm7pS_-HF0YfMg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:34 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59485', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'a88830af-beba-481c-9ef9-f98935230590', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0af34ad4c93f-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '515', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.252', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=382.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '254', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '-b-u5FT8QnpLhp5dyuRfSO8WbSFLmUhFjv4cFnY9mm7pS_-HF0YfMg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: a88830af-beba-481c-9ef9-f98935230590
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI 110001, IN.' took 0.57s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'zip': '11001', 'country_code': 'US', 'state_code': 'IN'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c71b8eb3-d680-49da-a5bd-cea29582292e', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Current AQI Kyoto, JP.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71338730>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C713388E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:39 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9bae3d7b-4809-4c75-aa79-56c0bae0dc40'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0b1659b0e5eb-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.176'), (b'fireworks-server-time-to-first-token', b'0.050'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=285.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'177'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'p7SVEnUfJ0-AFepu-NGnYbmv1kOWCKHo3pejBQT2Yhf4jFMt-nwWGg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:39 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9bae3d7b-4809-4c75-aa79-56c0bae0dc40', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0b1659b0e5eb-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.176', 'fireworks-server-time-to-first-token': '0.050', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=285.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '177', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'p7SVEnUfJ0-AFepu-NGnYbmv1kOWCKHo3pejBQT2Yhf4jFMt-nwWGg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9bae3d7b-4809-4c75-aa79-56c0bae0dc40
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Current AQI Kyoto, JP.' took 0.50s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Kyoto', 'country_code': 'JP'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8a84b9bc-3ac8-4eb5-97ae-18d96789a1eb', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality Toronto, CA now.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71309750>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7130B8B0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:45 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9f306502-b89d-41d8-ba77-f2ef366dc618'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0b38bb72c937-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.258'), (b'fireworks-server-time-to-first-token', b'0.068'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=372.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'260'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'odLdDrrpZEHE5IIM0PrKFAq1Qjq6XH_tKrhc33hG88M3ohvwFmA1ug==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:45 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9f306502-b89d-41d8-ba77-f2ef366dc618', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0b38bb72c937-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.258', 'fireworks-server-time-to-first-token': '0.068', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=372.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '260', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'odLdDrrpZEHE5IIM0PrKFAq1Qjq6XH_tKrhc33hG88M3ohvwFmA1ug=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9f306502-b89d-41d8-ba77-f2ef366dc618
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality Toronto, CA now.' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Toronto', 'country_code': 'CA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6a9227ad-e04e-4e6e-8058-420a7ef0388a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI Nairobi?'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71351C00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C713517E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:50 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'dde71230-6d9c-4132-b401-b25d77eff6ad'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0b5bec2f28a4-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.144'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=300.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'146'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'-GvAue7hJRzOBbfRTHabFB03oEupjqplKtTxokSXUgsGqAq_d4gnnw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:50 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'dde71230-6d9c-4132-b401-b25d77eff6ad', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0b5bec2f28a4-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.144', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=300.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '146', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '-GvAue7hJRzOBbfRTHabFB03oEupjqplKtTxokSXUgsGqAq_d4gnnw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: dde71230-6d9c-4132-b401-b25d77eff6ad
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI Nairobi?' took 0.55s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Nairobi'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a527f5a6-cfcb-4560-bc84-27a16591d283', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality in Auckland, NZ.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71353910>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C713534F0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 13:59:56 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'031475b6-99f1-4eda-8818-3948fd77edca'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0b7eeaed592f-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.200'), (b'fireworks-server-time-to-first-token', b'0.041'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=503.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'201'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 191181f299c93f856cc1cdad79c1bb76.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'r96tDdHt59XejoMiw4KFbMxPhcKw2mYQOf8jmieBwzAyr0PUFecYEA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 13:59:56 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '031475b6-99f1-4eda-8818-3948fd77edca', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0b7eeaed592f-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.200', 'fireworks-server-time-to-first-token': '0.041', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=503.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '201', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 191181f299c93f856cc1cdad79c1bb76.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'r96tDdHt59XejoMiw4KFbMxPhcKw2mYQOf8jmieBwzAyr0PUFecYEA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 031475b6-99f1-4eda-8818-3948fd77edca
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality in Auckland, NZ.' took 0.79s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Auckland', 'country_code': 'NZ'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-44c937f3-760b-41d3-bc33-4bc2b2ffaced', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI Reykjavik, IS (sorry for prev typo).'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7136D660>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7125DEA0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:02 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59479'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'816b89b9-f9db-496b-a2e8-cb58d87417ad'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0ba47c1fec03-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'521'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.174'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=301.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'175'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'pdNsDnjnVUWuQ7UaUrOgezUGjU9QAeQOcutqGD7kN029glWpitda1Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:02 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59479', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '816b89b9-f9db-496b-a2e8-cb58d87417ad', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0ba47c1fec03-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '521', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.174', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=301.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '175', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'pdNsDnjnVUWuQ7UaUrOgezUGjU9QAeQOcutqGD7kN029glWpitda1Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 816b89b9-f9db-496b-a2e8-cb58d87417ad
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI Reykjavik, IS (sorry for prev typo).' took 0.82s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Reykjavik', 'country_code': 'IS'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-453562ae-2652-472e-b7d6-ecc9dddb6c0f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Pollution level Lima, PE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:08 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'0fda82d9-1214-4f14-85c8-51669f05b470'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0bc75a51e62b-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.296'), (b'fireworks-server-time-to-first-token', b'0.062'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=416.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'298'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'WAO-859NOSWwRNnA5z3at8bS5D52UJD-ojhdd_8KoYSttZs1QSc6ZA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:08 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '0fda82d9-1214-4f14-85c8-51669f05b470', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0bc75a51e62b-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.296', 'fireworks-server-time-to-first-token': '0.062', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=416.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '298', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 a1a9ff59f73590e3953b5ce6edfc8aa8.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'WAO-859NOSWwRNnA5z3at8bS5D52UJD-ojhdd_8KoYSttZs1QSc6ZA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 0fda82d9-1214-4f14-85c8-51669f05b470
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Pollution level Lima, PE.' took 0.64s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Lima', 'country_code': 'PE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-20763aea-f616-4583-906d-c6d0b7cd6191', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI Johannesburg, ZA.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C713515A0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71351660>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:14 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59486'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'dbb648e0-1f08-4d20-b010-d07962900f42'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0bef5ce3ef65-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'514'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.168'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=362.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'170'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'41uq1a5XQXX7LdXCGVpxPR0G1VvkNLeGkkrEhxv0rqUh4bn4zbizVg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:14 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59486', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'dbb648e0-1f08-4d20-b010-d07962900f42', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0bef5ce3ef65-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '514', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.168', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=362.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '170', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '41uq1a5XQXX7LdXCGVpxPR0G1VvkNLeGkkrEhxv0rqUh4bn4zbizVg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: dbb648e0-1f08-4d20-b010-d07962900f42
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI Johannesburg, ZA.' took 1.39s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Johannesburg', 'country_code': 'ZA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9c4ef4c6-7d6e-48ef-84c7-2c9e03ea8ffe', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality Munich, DE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:19 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59487'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'5c744cee-8169-4742-827c-a073df8db0ff'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0c125801d6bf-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'513'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.197'), (b'fireworks-server-time-to-first-token', b'0.046'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=300.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'199'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'5PnPi-ci6XFy6MaIUvAF_gCDvIsV2oGecrftx284NA_xkbGxxZCHVQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:19 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59487', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '5c744cee-8169-4742-827c-a073df8db0ff', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0c125801d6bf-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '513', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.197', 'fireworks-server-time-to-first-token': '0.046', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=300.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '199', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 9bc25d3cccecc51547f094bc2aa70ef4.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '5PnPi-ci6XFy6MaIUvAF_gCDvIsV2oGecrftx284NA_xkbGxxZCHVQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 5c744cee-8169-4742-827c-a073df8db0ff
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality Munich, DE.' took 0.49s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Munich', 'country_code': 'DE'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-280177ff-5690-4861-ad0f-b16c8387b059', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'AQI San Francisco, US-CA.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7136FEB0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C7136FA90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:25 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59484'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'12e9b00d-dfb5-427b-87f7-91d30f4daa24'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0c351941f88d-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'516'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.278'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=445.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'281'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'gi4sOJuapV-s_xb6Urn_W1nnxjEkWWN24pdXma6rDG19-zbVI1XxrQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:25 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59484', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '12e9b00d-dfb5-427b-87f7-91d30f4daa24', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0c351941f88d-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '516', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.278', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=445.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '281', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'gi4sOJuapV-s_xb6Urn_W1nnxjEkWWN24pdXma6rDG19-zbVI1XxrQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 12e9b00d-dfb5-427b-87f7-91d30f4daa24
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'AQI San Francisco, US-CA.' took 0.71s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'San Francisco', 'country_code': 'US', 'state_code': 'CA'})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f53dcb6a-e558-4ab7-9b20-0065174d52f9', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Air quality Porto, PT.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C71391C00>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022C6E4F98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022C713917E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:00:31 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59487'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'3114b89f-b3ea-44fa-a4d0-7d895451a003'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b0c58b8aa7fed-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'513'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.202'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=574.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'204'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'oGJihNeFuA9u30M27VGz1S_L8EFVy41KZLxajeM94-VRy8ds2_f_Pg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:00:31 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59487', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '3114b89f-b3ea-44fa-a4d0-7d895451a003', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b0c58b8aa7fed-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '513', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.202', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=574.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '204', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'oGJihNeFuA9u30M27VGz1S_L8EFVy41KZLxajeM94-VRy8ds2_f_Pg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 3114b89f-b3ea-44fa-a4d0-7d895451a003
DEBUG    test_performance:test_performance.py:466 Current air pollution conversation 'Air quality Porto, PT.' took 0.82s with response: 
LLMResponse(type='function_call', content=None, function='get_air_pollution', module='meteorology', arguments={'city': 'Porto', 'country_code': 'PT'})


