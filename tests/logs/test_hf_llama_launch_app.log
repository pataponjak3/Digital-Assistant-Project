DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-337e22b9-32c2-4fa0-a05a-d788320929a8', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Steam.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33484C5E0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33484C310>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:30 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c0f7bd54-e95a-43bb-9398-c8f5fb52a127'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b13a51870c57b-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.198'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=452.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'199'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'EMra6-6vSfDuIWrqwcjmPjvFCW_okCbCwyJu-eXP1BxCSrTFttZdsw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:30 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c0f7bd54-e95a-43bb-9398-c8f5fb52a127', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b13a51870c57b-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.198', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=452.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '199', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'EMra6-6vSfDuIWrqwcjmPjvFCW_okCbCwyJu-eXP1BxCSrTFttZdsw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c0f7bd54-e95a-43bb-9398-c8f5fb52a127
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Steam.' took 1.17s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Steam', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8de15342-5d9e-41d7-90ef-0cc34238d0ab', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch dis cord.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33484E350>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33484E0E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:35 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'80159e69-543d-4c13-b09b-47d8ac96717f'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b13c8e8605187-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.254'), (b'fireworks-server-time-to-first-token', b'0.085'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=387.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'255'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'1tzO4Jaa5Q71_JefOlcgLs8htOMJ_YB-_i72NE6sz-nuTsV5rbl4Xw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:35 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '80159e69-543d-4c13-b09b-47d8ac96717f', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b13c8e8605187-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.254', 'fireworks-server-time-to-first-token': '0.085', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=387.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '255', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '1tzO4Jaa5Q71_JefOlcgLs8htOMJ_YB-_i72NE6sz-nuTsV5rbl4Xw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 80159e69-543d-4c13-b09b-47d8ac96717f
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch dis cord.' took 0.67s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Discord', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-03ff83b2-aeaf-478d-81cd-ceca4b46d817', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Eclipse IDE.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:41 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'7e2a4353-f102-4119-ada2-351aa65b4ebd'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b13ec1aaa3b36-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.220'), (b'fireworks-server-time-to-first-token', b'0.048'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=351.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'222'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'rBTf8BqEN3AGjzFxfn55XhDzXAgLlWPZC8RICAyu7cVwSfA4FIlXGw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:41 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '7e2a4353-f102-4119-ada2-351aa65b4ebd', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b13ec1aaa3b36-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.220', 'fireworks-server-time-to-first-token': '0.048', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=351.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '222', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'rBTf8BqEN3AGjzFxfn55XhDzXAgLlWPZC8RICAyu7cVwSfA4FIlXGw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 7e2a4353-f102-4119-ada2-351aa65b4ebd
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start Eclipse IDE.' took 0.63s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Eclipse IDE', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b25f61b7-e1e9-48d5-ad9f-2251f5ef6e65', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open VS Code.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334895B10>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348956F0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:47 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'73158d1d-3058-45ec-8a83-45531c76c47e'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b140f9dedd6a3-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.186'), (b'fireworks-server-time-to-first-token', b'0.053'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=349.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'187'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'eed9wsOxEvPat3WxDVrEtWJHUwjDZTMW-PrbGgXRO6ulrhru_TJmFw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:47 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '73158d1d-3058-45ec-8a83-45531c76c47e', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b140f9dedd6a3-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.186', 'fireworks-server-time-to-first-token': '0.053', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=349.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '187', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 c6ccd07e1e50408d404ed1f9dd2506ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'eed9wsOxEvPat3WxDVrEtWJHUwjDZTMW-PrbGgXRO6ulrhru_TJmFw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 73158d1d-3058-45ec-8a83-45531c76c47e
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open VS Code.' took 0.59s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Visual Studio Code', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6d905837-65c4-4e26-96dd-3ccb5ab4767a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Run visual studio code pls.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334897820>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334897400>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:52 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59487'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'199835b5-2631-409b-922f-2e3a921fd556'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b143258c8821e-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'513'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.275'), (b'fireworks-server-time-to-first-token', b'0.070'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=408.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'278'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f13924e40949c7e0a5bd0c7e333695f2.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'i1N3-jlxhE9N6xMslHIkXn62kCOsU_uwusaRJYwy7OXg_74rj7No-w==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:52 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59487', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '199835b5-2631-409b-922f-2e3a921fd556', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b143258c8821e-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '513', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.275', 'fireworks-server-time-to-first-token': '0.070', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=408.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '278', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f13924e40949c7e0a5bd0c7e333695f2.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'i1N3-jlxhE9N6xMslHIkXn62kCOsU_uwusaRJYwy7OXg_74rj7No-w=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 199835b5-2631-409b-922f-2e3a921fd556
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Run visual studio code pls.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Visual Studio Code', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ff3aeadd-97b4-4e1c-b0aa-26a429cf024e', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch VSC.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334895870>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334895180>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:05:58 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c64bbe96-efe0-431b-b8db-e6378fef418b'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1455b87774e8-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.252'), (b'fireworks-server-time-to-first-token', b'0.082'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=362.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'253'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'AlWOMQX1Vd2aLLO8HMdieYHgjn86ejHT7P5JuYlFzbFcJ_3f2cmEfw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:05:58 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c64bbe96-efe0-431b-b8db-e6378fef418b', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1455b87774e8-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.252', 'fireworks-server-time-to-first-token': '0.082', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=362.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '253', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'AlWOMQX1Vd2aLLO8HMdieYHgjn86ejHT7P5JuYlFzbFcJ_3f2cmEfw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c64bbe96-efe0-431b-b8db-e6378fef418b
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch VSC.' took 0.56s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Visual Studio Code', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3dcb82cb-d18d-49e0-a1ab-5569cc0bd691', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Win RAR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:03 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59488'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'c570dcf2-9d88-49a8-ba58-11217b152d3d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b14784cd1e642-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'512'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.173'), (b'fireworks-server-time-to-first-token', b'0.041'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=313.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'175'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'aHzXFKdu4s3yLSUlaMwkcCpP8bH3Y02Y9jO13-_sWo2wCzk3Fv6yIg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:03 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59488', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'c570dcf2-9d88-49a8-ba58-11217b152d3d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b14784cd1e642-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '512', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.173', 'fireworks-server-time-to-first-token': '0.041', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=313.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '175', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'aHzXFKdu4s3yLSUlaMwkcCpP8bH3Y02Y9jO13-_sWo2wCzk3Fv6yIg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: c570dcf2-9d88-49a8-ba58-11217b152d3d
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Win RAR.' took 0.48s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Win RAR', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c5910576-69a9-4ed1-9ac9-cb0b1b2f897c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start WinRAR.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348C2620>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348C2200>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:09 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'6c1f2e79-d504-49dc-ac99-4d8d72d31844'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b149aefe73b41-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.228'), (b'fireworks-server-time-to-first-token', b'0.046'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=337.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'230'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'zp5U0BeNGQY6494ZKcHRR913q3yojnX634g5ViWaWmAEy8UvsAyyPw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:09 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '6c1f2e79-d504-49dc-ac99-4d8d72d31844', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b149aefe73b41-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.228', 'fireworks-server-time-to-first-token': '0.046', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=337.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '230', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'zp5U0BeNGQY6494ZKcHRR913q3yojnX634g5ViWaWmAEy8UvsAyyPw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 6c1f2e79-d504-49dc-ac99-4d8d72d31844
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start WinRAR.' took 0.56s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'WinRAR', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a008ac85-836e-425f-a8f5-876ab9765665', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Google Chorme.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348E0370>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348C3F10>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:15 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59487'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'497fefad-eb23-41ce-b2b3-80a78864942c'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b14be1fead692-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'513'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.191'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=286.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'193'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'ujRtqvqWoQBCNRBfO_dgKE-OCVa2cu96r8yXR0nxbkNgpVqfTfOV4Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:15 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59487', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '497fefad-eb23-41ce-b2b3-80a78864942c', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b14be1fead692-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '513', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.191', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=286.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '193', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 f355844b811a4a5ec94df0918f0fb80c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'ujRtqvqWoQBCNRBfO_dgKE-OCVa2cu96r8yXR0nxbkNgpVqfTfOV4Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 497fefad-eb23-41ce-b2b3-80a78864942c
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Google Chorme.' took 0.55s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Google Chrome', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0a327e3c-4e5a-4598-87e4-43c1df60ce20', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch chrome browser.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348E2080>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33484DA50>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:20 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'131b3f84-9377-44e6-a803-e58a74e69ede'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b14e07ffd083a-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.266'), (b'fireworks-server-time-to-first-token', b'0.083'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=548.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'268'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'dBDtmhWRUxBEVnNxCGp8RgADrDCb3KysN8zpHNpq05LiZ6-7BQt1Ng==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:20 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '131b3f84-9377-44e6-a803-e58a74e69ede', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b14e07ffd083a-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.266', 'fireworks-server-time-to-first-token': '0.083', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=548.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '268', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2c1fa76ea6af6e9212cf3e52e166c4ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'dBDtmhWRUxBEVnNxCGp8RgADrDCb3KysN8zpHNpq05LiZ6-7BQt1Ng=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 131b3f84-9377-44e6-a803-e58a74e69ede
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch chrome browser.' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'chrome', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1a2c7e90-aef5-4cb6-a81a-1b79c446ea1a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Firefox.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348C0940>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348C21D0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:26 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'ac1df711-e865-4f9f-b0c7-e804baacfe1a'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1504ab9ac9a8-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.187'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=286.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'188'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Zxi7fnfa9uosdK20QX0ZGKTrMOl7ctiM3uAJt5fIqT8IdpFw8ok-6Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:26 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'ac1df711-e865-4f9f-b0c7-e804baacfe1a', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1504ab9ac9a8-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.187', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=286.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '188', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Zxi7fnfa9uosdK20QX0ZGKTrMOl7ctiM3uAJt5fIqT8IdpFw8ok-6Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: ac1df711-e865-4f9f-b0c7-e804baacfe1a
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Firefox.' took 0.51s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Firefox', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-515026a7-fa2c-4eb7-b2f4-3b4e9c2f336c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Notepad.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:31 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'936dc429-c300-4665-887e-d3840c0e6b02'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1526b8ce0813-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.292'), (b'fireworks-server-time-to-first-token', b'0.138'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=419.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'295'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'EO8kgdjlJUMTJaSreN7R_Uv2QbU1bNq7UTHTZMbkL2EBqOAAruKdRw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:31 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '936dc429-c300-4665-887e-d3840c0e6b02', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1526b8ce0813-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.292', 'fireworks-server-time-to-first-token': '0.138', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=419.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '295', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 2982304f906b538b26d5fd128f6030ec.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'EO8kgdjlJUMTJaSreN7R_Uv2QbU1bNq7UTHTZMbkL2EBqOAAruKdRw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 936dc429-c300-4665-887e-d3840c0e6b02
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start Notepad.' took 0.59s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Notepad', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a993d0a6-f07e-4182-b303-8a084d8008b4', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch calc.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348FC0D0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348E3C70>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:37 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'8c0f7847-c966-4b08-8d68-f785d3672f66'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b154a4cda05c4-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.245'), (b'fireworks-server-time-to-first-token', b'0.095'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=363.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'247'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 56d84f665e9029878cb3adcd83a21026.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'zrfcS0yqTqjTkT2VLl4OU8F3CkB0494ZIcoEYfGS17_JfxAqCun3EA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:37 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '8c0f7847-c966-4b08-8d68-f785d3672f66', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b154a4cda05c4-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.245', 'fireworks-server-time-to-first-token': '0.095', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=363.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '247', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 56d84f665e9029878cb3adcd83a21026.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'zrfcS0yqTqjTkT2VLl4OU8F3CkB0494ZIcoEYfGS17_JfxAqCun3EA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 8c0f7847-c966-4b08-8d68-f785d3672f66
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch calc.' took 0.65s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'calc', 'is_sure_after_multiple_matches': True})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-746adf45-9aca-440a-8104-0b71ca50ee18', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open VLC.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348FDDE0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348FD9C0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:43 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'be7395a2-bc87-498c-9bb1-219ba2a66926'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b156d6d6f8f17-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.322'), (b'fireworks-server-time-to-first-token', b'0.046'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=440.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'324'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'tLIAUqVCmtIdYiykZoyLTQwPte0Y2vpfPzHEe2wVTm3qKR7QNY5wQg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:43 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'be7395a2-bc87-498c-9bb1-219ba2a66926', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b156d6d6f8f17-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.322', 'fireworks-server-time-to-first-token': '0.046', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=440.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '324', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'tLIAUqVCmtIdYiykZoyLTQwPte0Y2vpfPzHEe2wVTm3qKR7QNY5wQg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: be7395a2-bc87-498c-9bb1-219ba2a66926
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open VLC.' took 0.66s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'VLC', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cb9b9629-1bb4-4d0d-9eb0-73d8c5ea04d3', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Spotify.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:48 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'4d421d4d-259d-4fd2-9f5f-fdf4c9997610'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b15907852083a-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.173'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=300.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'174'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'B30DlaJbTij5hf_xBXZd1t-Q7jbSejk43D4Q1-RC2Bsp2hqA3wS_1A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:48 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '4d421d4d-259d-4fd2-9f5f-fdf4c9997610', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b15907852083a-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.173', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=300.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '174', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 d8d835cce198f21656f532aa7cb25fbe.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'B30DlaJbTij5hf_xBXZd1t-Q7jbSejk43D4Q1-RC2Bsp2hqA3wS_1A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 4d421d4d-259d-4fd2-9f5f-fdf4c9997610
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start Spotify.' took 0.47s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Spotify', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f9642c4d-bf1a-495f-b8ae-c08618538d1c', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Adobe Reader.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348E1270>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348E29E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:54 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'21f53091-4155-45bb-8c5e-28f2b405fbbc'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b15b30bd13b5a-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.239'), (b'fireworks-server-time-to-first-token', b'0.106'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=384.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'241'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'A_UHUVTCWL-Ihwti2sxkDpNaubTbdR0bb2Uj65snrm6xJGZqf7Fh2Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:54 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '21f53091-4155-45bb-8c5e-28f2b405fbbc', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b15b30bd13b5a-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.239', 'fireworks-server-time-to-first-token': '0.106', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=384.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '241', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'A_UHUVTCWL-Ihwti2sxkDpNaubTbdR0bb2Uj65snrm6xJGZqf7Fh2Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 21f53091-4155-45bb-8c5e-28f2b405fbbc
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Adobe Reader.' took 0.61s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Adobe Reader', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6c93a92c-acb2-4679-bc2d-a7cfcbc5f116', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Acrobat.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:06:59 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9c04e29f-6249-495b-a813-fe64cfb495d0'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b15d60e29827b-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.166'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=278.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'167'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'bLYw5CKxQ_j_u0zDzvSC8AB490RewQTVWHNOxtChbzoDpSoHbss3MA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:06:59 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9c04e29f-6249-495b-a813-fe64cfb495d0', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b15d60e29827b-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.166', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=278.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '167', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'bLYw5CKxQ_j_u0zDzvSC8AB490RewQTVWHNOxtChbzoDpSoHbss3MA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9c04e29f-6249-495b-a813-fe64cfb495d0
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch Acrobat.' took 0.50s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Acrobat', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-abf0b62c-4545-4542-9d92-de56301dd7e0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Slack.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334928D30>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334928910>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:05 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'806127bd-b884-47b2-8262-7513f14834dd'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b15f88b542426-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.468'), (b'fireworks-server-time-to-first-token', b'0.042'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=601.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'470'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'4GcSCLa2tRWoh8aH_tJu_V9enf88zfzOy7-M4IoS9gdZ7b4_IDJ8Kg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:05 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '806127bd-b884-47b2-8262-7513f14834dd', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b15f88b542426-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.468', 'fireworks-server-time-to-first-token': '0.042', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=601.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '470', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 87bac7324deaa405623c690127b8a87c.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '4GcSCLa2tRWoh8aH_tJu_V9enf88zfzOy7-M4IoS9gdZ7b4_IDJ8Kg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 806127bd-b884-47b2-8262-7513f14834dd
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Slack.' took 0.84s with response: 
LLMResponse(type='response', content="I'm not capable of opening specific applications like Slack. However, I can help you with that. To open Slack, you can try the following:\n\n1. Check if Slack is already installed on your system. If it's installed, you can search for it in the Start Menu or Applications folder.\n2. If Slack is not installed, you can download and install it from the Slack website.\n3. Once installed, you can launch Slack by searching for it in the Start Menu or Applications folder.\n\nWould you like me to help you with installing or launching Slack?", function=None, module=None, arguments=None)


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2ad24579-1439-41ba-9900-ccb19a46139b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start MS Teams.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33492AA40>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33492A620>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:11 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'90968588-096e-477a-ae9d-5a7fd3b08957'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b161d2fa5fbd0-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.184'), (b'fireworks-server-time-to-first-token', b'0.049'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=300.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'185'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'zGM_hueswxuTQrXMKYnSm4oNa75x2oT_1_4mrQGmzDdlQpv7Kp5HoA==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:11 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '90968588-096e-477a-ae9d-5a7fd3b08957', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b161d2fa5fbd0-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.184', 'fireworks-server-time-to-first-token': '0.049', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=300.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '185', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'zGM_hueswxuTQrXMKYnSm4oNa75x2oT_1_4mrQGmzDdlQpv7Kp5HoA=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 90968588-096e-477a-ae9d-5a7fd3b08957
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start MS Teams.' took 0.55s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'MS Teams', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-aadb8ed5-89f5-4311-aa58-cd9f8284ac55', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Outlok.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:16 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'5c0dcf58-04b3-4941-91e8-cc678ac4c19c'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b163f7b361216-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.165'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=300.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'166'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'dxH_CLwq0-tpQhNJZl-yVOwWmmCwcyNbsVUy8Pvv2oRdNgUYL2sn_A==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:16 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '5c0dcf58-04b3-4941-91e8-cc678ac4c19c', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b163f7b361216-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.165', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=300.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '166', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'dxH_CLwq0-tpQhNJZl-yVOwWmmCwcyNbsVUy8Pvv2oRdNgUYL2sn_A=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 5c0dcf58-04b3-4941-91e8-cc678ac4c19c
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Outlok.' took 0.48s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Outlook', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0d0bcf3f-fc0b-4441-9467-6e11ad643136', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Word.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:22 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'dfaba9ea-212a-4db3-8e22-df1b3ca9ca70'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1661cd10379a-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.187'), (b'fireworks-server-time-to-first-token', b'0.043'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=403.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'189'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'6u2D95E14qFoT5PQVm3nvxefSOiHMydc7j0GKvLcG7n0HjQCe5MuXg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:22 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'dfaba9ea-212a-4db3-8e22-df1b3ca9ca70', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1661cd10379a-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.187', 'fireworks-server-time-to-first-token': '0.043', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=403.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '189', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 27a35654821ee52d8aa69c940ad5de7e.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': '6u2D95E14qFoT5PQVm3nvxefSOiHMydc7j0GKvLcG7n0HjQCe5MuXg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: dfaba9ea-212a-4db3-8e22-df1b3ca9ca70
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch Word.' took 0.60s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Word', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-52ae5ebf-3fdb-418e-af55-cbc0d8063a47', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Excel.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348FD390>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B3348FFE80>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:27 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'53579319-adb9-48f3-beac-029e63469b27'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b16852f975a1c-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.151'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=336.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'152'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'A_R5nF-uBVTdFy5qo5JlF2wmK92uq9E_8gVLLwhDsImoQ1N9_KXfEg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:27 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '53579319-adb9-48f3-beac-029e63469b27', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b16852f975a1c-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.151', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=336.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '152', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 42c937f806e6e43029a719b83b9a8612.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'A_R5nF-uBVTdFy5qo5JlF2wmK92uq9E_8gVLLwhDsImoQ1N9_KXfEg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 53579319-adb9-48f3-beac-029e63469b27
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Excel.' took 0.56s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Excel', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f2a1e9d4-1427-4898-b6ce-124d74889b07', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Power Point.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33493D900>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33493D4E0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:33 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'9eded756-da6b-4a43-a64a-3fa48e336d94'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b16a8eb8fe629-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.250'), (b'fireworks-server-time-to-first-token', b'0.088'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=384.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'252'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'rkkcYhJX2zAfJoZRtNwsCvsZQz5hSTflBsA9vRlaM9Mz8kTDYuOkew==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:33 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '9eded756-da6b-4a43-a64a-3fa48e336d94', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b16a8eb8fe629-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.250', 'fireworks-server-time-to-first-token': '0.088', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=384.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '252', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'rkkcYhJX2zAfJoZRtNwsCvsZQz5hSTflBsA9vRlaM9Mz8kTDYuOkew=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 9eded756-da6b-4a43-a64a-3fa48e336d94
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start Power Point.' took 0.76s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Power Point', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-625529ae-0ae0-48e6-99ae-08b8d045de43', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Paint.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:39 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'3e6e1afa-4372-48fd-b9dc-fd31e1122058'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b16cbeef61329-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.165'), (b'fireworks-server-time-to-first-token', b'0.040'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=281.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'167'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'lsgNMzYBSJEVa6TElUEp1oGnsWB-cdVrrbRBzeZdDPBwFz316Vy0Ew==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:39 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '3e6e1afa-4372-48fd-b9dc-fd31e1122058', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b16cbeef61329-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.165', 'fireworks-server-time-to-first-token': '0.040', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=281.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '167', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'lsgNMzYBSJEVa6TElUEp1oGnsWB-cdVrrbRBzeZdDPBwFz316Vy0Ew=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 3e6e1afa-4372-48fd-b9dc-fd31e1122058
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Paint.' took 0.51s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Paint', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5fd1324f-1179-49fd-9f7a-730da2d971ac', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start Edge browser.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334954F40>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334954B20>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:44 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'6fdd6148-b821-44f2-9c6b-2797f7802c95'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b16eece946906-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.169'), (b'fireworks-server-time-to-first-token', b'0.044'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=315.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'171'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b't5hcNWCIAQ8P-Cy_07pk1Kv8MkZnkq0L-8qMzcaUnXpCiFrZb2GPcQ==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:44 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '6fdd6148-b821-44f2-9c6b-2797f7802c95', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b16eece946906-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.169', 'fireworks-server-time-to-first-token': '0.044', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=315.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '171', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 7a968ba892318de9d85ba300078a49ce.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 't5hcNWCIAQ8P-Cy_07pk1Kv8MkZnkq0L-8qMzcaUnXpCiFrZb2GPcQ=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 6fdd6148-b821-44f2-9c6b-2797f7802c95
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start Edge browser.' took 0.64s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Edge', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2f16e91c-8e51-475e-8fbe-96e80d6e355a', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Git Extensions.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33493F460>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33493DA80>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:50 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'b3b3522c-c083-45db-bfc5-5d6a7cc72504'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1711bd2081e1-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.206'), (b'fireworks-server-time-to-first-token', b'0.043'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=335.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'208'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'yMwhUssqkJuZpJcjRAT2w38CZrES2aO3WwgTF6vtFrmYM43ClFk11Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:50 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'b3b3522c-c083-45db-bfc5-5d6a7cc72504', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1711bd2081e1-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.206', 'fireworks-server-time-to-first-token': '0.043', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=335.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '208', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 1a8de953aa4aaf678d8f6dfdeeea9a46.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'yMwhUssqkJuZpJcjRAT2w38CZrES2aO3WwgTF6vtFrmYM43ClFk11Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: b3b3522c-c083-45db-bfc5-5d6a7cc72504
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch Git Extensions.' took 0.55s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Git Extensions', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-bf0539bf-4afd-42bd-b514-dc6260feb16d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open 7Zip.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334928130>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33492AB30>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:07:55 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59488'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'4404852f-a461-4fdb-b6da-b81c28d82977'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b17347c05d6c4-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'512'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.167'), (b'fireworks-server-time-to-first-token', b'0.039'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=277.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'170'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'CkbJI7uuN20lgSlnqbMWW9ruyhrcJEL3bDyNdj4_fmEp6UruXgwpCg==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:07:55 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59488', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '4404852f-a461-4fdb-b6da-b81c28d82977', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b17347c05d6c4-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '512', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.167', 'fireworks-server-time-to-first-token': '0.039', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=277.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '170', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'CkbJI7uuN20lgSlnqbMWW9ruyhrcJEL3bDyNdj4_fmEp6UruXgwpCg=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 4404852f-a461-4fdb-b6da-b81c28d82977
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open 7Zip.' took 0.51s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': '7Zip', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5ff31356-e926-40a1-a850-b89374852cd7', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Start IntelliJ.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:08:01 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59490'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'7f9ab817-ed55-4b4a-b990-e7e6185a7409'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b1756add31216-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'510'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.168'), (b'fireworks-server-time-to-first-token', b'0.038'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=259.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'170'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'jOq2sJNBPHW2AfjTS_9A9mKwManiv1tjD7z_OSFq8JrsRff1hi5xtw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:08:01 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59490', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '7f9ab817-ed55-4b4a-b990-e7e6185a7409', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b1756add31216-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '510', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.168', 'fireworks-server-time-to-first-token': '0.038', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=259.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '170', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 55f14075e1cb487de38b7e615fd21a96.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'jOq2sJNBPHW2AfjTS_9A9mKwManiv1tjD7z_OSFq8JrsRff1hi5xtw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 7f9ab817-ed55-4b4a-b990-e7e6185a7409
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Start IntelliJ.' took 0.42s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'IntelliJ', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c072e116-620d-443c-ac78-3170f29738cd', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Open Pychram.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='router.huggingface.co' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B33497C1F0>
DEBUG    httpcore.connection:_trace.py:47 start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B331AA98C0> server_hostname='router.huggingface.co' timeout=5.0
DEBUG    httpcore.connection:_trace.py:47 start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B334957D90>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:08:07 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59488'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'8040cd57-9d4e-475c-8904-dfecba250dcc'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b17790ad59bb5-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'512'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.239'), (b'fireworks-server-time-to-first-token', b'0.103'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=576.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'240'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'Cgb3ebFMxf3Dm_HRTkKv5HATmTJMx2i2fsi31GSh0irrdqAAG1klzw==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:08:07 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59488', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': '8040cd57-9d4e-475c-8904-dfecba250dcc', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b17790ad59bb5-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '512', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.239', 'fireworks-server-time-to-first-token': '0.103', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=576.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '240', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'Cgb3ebFMxf3Dm_HRTkKv5HATmTJMx2i2fsi31GSh0irrdqAAG1klzw=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: 8040cd57-9d4e-475c-8904-dfecba250dcc
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Open Pychram.' took 0.81s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'PyCharm', 'is_sure_after_multiple_matches': False})


DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cf43d9a7-3f44-49bd-81bd-3063dc6c3682', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a digital assistant that can call predefined functions when they match the user\'s request.\n\nIMPORTANT RULES:\n- If the user’s input matches a function, reply ONLY with a JSON object in this format:\n{\n  "module": "<module_name>",\n  "function": "<function_name>",\n  "arguments": { "key": "value" }\n}\nDo not include explanations, disclaimers, or any other text.\n- If the user’s input does NOT match a function, reply naturally.\n\nAvailable functions:\n\nFunction: get_current_weather\nModule: meteorology\nDescription: Get the current weather for a location.\nArguments:\n- city (string): City name. Optionally add state_code (only for USA) and country_code (follows ISO 3166).\n- lat (number) and lon (number): Coordinates (-90 to 90, -180 to 180).\n- zip (string) and country_code (string): Postal code and country code.\n- units (string): Optional. Temperature unit: standard, metric, imperial.\n- lang (string): Optional. Language code for weather description (e.g., \'en\').\n\nFunction: get_forecast\nModule: meteorology\nDescription: Get a 5-day forecast in 3-hour intervals.\nArguments: Same as get_current_weather.\n\nFunction: get_air_pollution\nModule: meteorology\nDescription: Get current air pollution data for a location.\nArguments: Same as get_current_weather.\n\nFunction: launch_application\nModule: os\nDescription: Launches an application by its name in the system, if it has a shortcut in the Start Menu.\nArguments:\n- app_name (string): The name of the application to launch.\n- is_sure_after_multiple_matches (bool): Indicates if the user confirmed which application to launch when multiple matches were found before in the conversation.\n\n\nOnly respond with a JSON object when the user’s message clearly requires calling a function (like requesting weather or pollution data). For example, "What’s the weather in Lisbon?" would match a function.\n\nIf the user refers to earlier data or asks follow-up questions about information you already returned (e.g., “What can you tell me about this data?”), answer NATURALLY and DO NOT generate a JSON object.\n\nNever reflect on how you should have responded unless told otherwise. Focus on answering the user’s latest question or request clearly and helpfully, using the information you already provided if relevant.'}, {'role': 'user', 'content': 'Launch Android Studio.'}], 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'temperature': 0.7, 'top_p': 0.9}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST https://router.huggingface.co/v1/chat/completions
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Sep 2025 14:08:12 GMT'), (b'x-ratelimit-remaining-tokens-generated', b'12000'), (b'x-ratelimit-remaining-tokens-prompt', b'59489'), (b'X-Powered-By', b'huggingface-moon'), (b'vary', b'Accept-Encoding'), (b'access-control-allow-origin', b'*'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Robots-Tag', b'none'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'x-request-id', b'dbec53e8-905d-4f30-89ce-a3d03c681e6d'), (b'x-inference-provider', b'fireworks-ai'), (b'cf-cache-status', b'DYNAMIC'), (b'cf-ray', b'984b179cfad1d6ff-IAD'), (b'fireworks-middleware-version', b'v2'), (b'fireworks-prompt-tokens', b'511'), (b'fireworks-sampling-options', b'{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}'), (b'fireworks-server-processing-time', b'0.225'), (b'fireworks-server-time-to-first-token', b'0.047'), (b'fireworks-speculation-prompt-matched-tokens', b'0'), (b'server', b'cloudflare'), (b'server-timing', b'total;dur=343.0;desc="Total Response Time"'), (b'x-envoy-upstream-service-time', b'226'), (b'x-ratelimit-limit-requests', b'60'), (b'x-ratelimit-limit-tokens-generated', b'12000'), (b'x-ratelimit-limit-tokens-prompt', b'60000'), (b'x-ratelimit-over-limit', b'no'), (b'x-ratelimit-remaining-requests', b'59'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'LIS50-P1'), (b'X-Amz-Cf-Id', b'pd7vSz-pdFoFwwIZPgxQGq1yDUs5AfCSicJORvR6N8SP91zDcLbg3Q==')])
INFO     httpx:_client.py:1025 HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST https://router.huggingface.co/v1/chat/completions "200 OK" Headers({'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'date': 'Thu, 25 Sep 2025 14:08:12 GMT', 'x-ratelimit-remaining-tokens-generated': '12000', 'x-ratelimit-remaining-tokens-prompt': '59489', 'x-powered-by': 'huggingface-moon', 'vary': 'Accept-Encoding', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash', 'x-robots-tag': 'none', 'cross-origin-opener-policy': 'same-origin', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-request-id': 'dbec53e8-905d-4f30-89ce-a3d03c681e6d', 'x-inference-provider': 'fireworks-ai', 'cf-cache-status': 'DYNAMIC', 'cf-ray': '984b179cfad1d6ff-IAD', 'fireworks-middleware-version': 'v2', 'fireworks-prompt-tokens': '511', 'fireworks-sampling-options': '{"max_tokens": 2048, "temperature": 0.7, "top_k": 0, "top_p": 0.9, "min_p": 0.0, "typical_p": 1.0, "frequency_penalty": 0.0, "presence_penalty": 0.0, "repetition_penalty": 1.0, "mirostat_target": null, "mirostat_lr": 0.1}', 'fireworks-server-processing-time': '0.225', 'fireworks-server-time-to-first-token': '0.047', 'fireworks-speculation-prompt-matched-tokens': '0', 'server': 'cloudflare', 'server-timing': 'total;dur=343.0;desc="Total Response Time"', 'x-envoy-upstream-service-time': '226', 'x-ratelimit-limit-requests': '60', 'x-ratelimit-limit-tokens-generated': '12000', 'x-ratelimit-limit-tokens-prompt': '60000', 'x-ratelimit-over-limit': 'no', 'x-ratelimit-remaining-requests': '59', 'x-cache': 'Miss from cloudfront', 'via': '1.1 5d25c31f47a198dbf50acf297a389a00.cloudfront.net (CloudFront)', 'x-amz-cf-pop': 'LIS50-P1', 'x-amz-cf-id': 'pd7vSz-pdFoFwwIZPgxQGq1yDUs5AfCSicJORvR6N8SP91zDcLbg3Q=='})
DEBUG    openai._base_client:_base_client.py:1024 request_id: dbec53e8-905d-4f30-89ce-a3d03c681e6d
DEBUG    test_performance:test_performance.py:483 Launch application conversation 'Launch Android Studio.' took 0.53s with response: 
LLMResponse(type='function_call', content=None, function='launch_application', module='os', arguments={'app_name': 'Android Studio', 'is_sure_after_multiple_matches': False})


